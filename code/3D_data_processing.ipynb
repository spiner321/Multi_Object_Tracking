{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 모듈 및 함수\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle as pkl\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "# 문자열 숫자리스트로 바꾸는 함수\n",
    "def str2list(txt):\n",
    "    txt = txt.replace('\\n', '').split(',')\n",
    "    txt = list(map(float, txt))\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "# 리스트를 문자열로 바꾸는 함수\n",
    "def list2str(list):\n",
    "    list = ' '.join(map(str, list))\n",
    "    \n",
    "    return list\n",
    "\n",
    "\n",
    "# alpha 구하는 공식\n",
    "import math\n",
    "\n",
    "def normalizeAngle(angle):\n",
    "    result = angle % (2*math.pi)\n",
    "    if result < -math.pi:\n",
    "        result += 2*math.pi\n",
    "    elif result > math.pi:\n",
    "        result -= 2*math.pi\n",
    "    return result\n",
    "\n",
    "def cal_alpha_ori(x, z, ry):  \n",
    "    angle = ry\n",
    "    angle -= -math.atan2(z, x) -1.5*math.pi \n",
    "    alpha = normalizeAngle(angle)\n",
    "    return alpha # -1.818032754845337\n",
    "# cal_alpha_ori(2.5702, 9.7190, -1.5595)\n",
    "\n",
    "\n",
    "# convert 2d coordinate\n",
    "def xyxy2xywhn(x, w=1920, h=1200, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    if clip:\n",
    "        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    y = list(y.reshape(-1))\n",
    "    return y\n",
    "\n",
    "# rotation matrix\n",
    "def roty(t, Rx=90/180*np.pi):\n",
    "    ''' Rotation about the y-axis. '''\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "    # return  np.array([[c, 0, s],\n",
    "    #                 [0, 1, 0],\n",
    "    #                 [-s, 0, c]])\n",
    "\n",
    "    X = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(Rx), -np.sin(Rx)],\n",
    "                    [0, np.sin(Rx), np.cos(Rx)]])\n",
    "\n",
    "    # X = np.eye(3)\n",
    "\n",
    "    Z = np.array([[c, -s, 0],\n",
    "                    [s, c, 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    return np.matmul(Z, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # kitti dataset calib 확인\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/hwang/datasets/kitti/training/calib/000000.txt', 'r') as f:\n",
    "    txt = f.readlines()\n",
    "\n",
    "p0 = txt[0].replace('\\n', '').split(' ')[1:] # 0번 카메라, projection matrix (= intrinsic)\n",
    "p1 = txt[1].replace('\\n', '').split(' ')[1:] # 1번 카메라\n",
    "p2 = txt[2].replace('\\n', '').split(' ')[1:] # 2번 카메라 (모델에 이 정보를 사용), (i, 4)위치는 기본 카메라와의 위상 차이 (각 x, y, z 축)\n",
    "p3 = txt[3].replace('\\n', '').split(' ')[1:] # 3번 카메라\n",
    "R_rect = txt[4].replace('\\n', '').split(' ')[1:] # 스테레오 카메라로 촬영 시 rotation matrix 보정 수치 (모노 카메라면 단위행렬 사용)\n",
    "Tr_velo = txt[5].replace('\\n', '').split(' ')[1:] # extrinsic (사용은 rigid body transformation 형태로 사용, extrinsic 4행에 [0, 0, 0, 1]을 추가)\n",
    "Tr_imu = txt[6].replace('\\n', '').split(' ')[1:]\n",
    "\n",
    "print(np.around(np.asarray(p0, dtype=float).reshape(-1, 4), 1), '\\n')\n",
    "print(np.around(np.asarray(p1, dtype=float).reshape(-1, 4), 1), '\\n')\n",
    "print(np.around(np.asarray(p2, dtype=float).reshape(-1, 4), 1), '\\n')\n",
    "print(np.around(np.asarray(p3, dtype=float).reshape(-1, 4), 1), '\\n')\n",
    "print(np.asarray(R_rect, dtype=float).reshape(-1, 3), '\\n')\n",
    "print(np.asarray(Tr_velo, dtype=float).reshape(-1, 4), '\\n')\n",
    "print(np.asarray(Tr_imu, dtype=float).reshape(-1, 4), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_json = {\n",
    "    \"extrinsic\": np.asarray(Tr_velo, dtype=np.float32).tolist(),\n",
    "    \"intrinsic\": np.asarray(p2, dtype=np.float32).reshape(-1, 4)[:3, :3].flatten().tolist()\n",
    "}\n",
    "json_object = json.dumps(calib_json, indent = 4) \n",
    "# Writing to sample.json \n",
    "with open('/data/NIA50/SUSTechPOINTS_2-050/data/nia48/calib/000055.json', \"w\") as outfile: \n",
    "    outfile.write(json_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_path = '/data/hwang/datasets/kitti/training/calib/'\n",
    "frame = '000000'\n",
    "\n",
    "def get_inv_matrix(file, v2c, rect):\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "        trans = [x for x in filter(lambda s: s.startswith(v2c), lines)][0]\n",
    "        \n",
    "        matrix = [m for m in map(lambda x: float(x), trans.strip().split(\" \")[1:])]\n",
    "        matrix = matrix + [0,0,0,1]\n",
    "        m = np.array(matrix)\n",
    "        velo_to_cam  = m.reshape([4,4])\n",
    "\n",
    "\n",
    "        trans = [x for x in filter(lambda s: s.startswith(rect), lines)][0]\n",
    "        matrix = [m for m in map(lambda x: float(x), trans.strip().split(\" \")[1:])]        \n",
    "        m = np.array(matrix).reshape(3,3)\n",
    "        \n",
    "        m = np.concatenate((m, np.expand_dims(np.zeros(3), 1)), axis=1)\n",
    "        \n",
    "        rect = np.concatenate((m, np.expand_dims(np.array([0,0,0,1]), 0)), axis=0)        \n",
    "        \n",
    "        # print(velo_to_cam, rect)    \n",
    "        m = np.matmul(rect, velo_to_cam)\n",
    "\n",
    "\n",
    "        m = np.linalg.inv(m)\n",
    "        \n",
    "        return m\n",
    "        \n",
    "def get_detection_inv_matrix(calib_path, frame):\n",
    "    file = os.path.join(calib_path, frame+\".txt\")\n",
    "    return get_inv_matrix(file, \"Tr_velo_to_cam\", \"R0_rect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # convert pcd - npy\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # pcd to npy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = o3d.core.Device('cuda:1')\n",
    "pcd_f = o3d.t.io.read_point_cloud('/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_0000/lidar/0000.pcd')\n",
    "print(pcd_f)\n",
    "\n",
    "positions = pcd_f.point.positions.numpy()\n",
    "intensity = pcd_f.point.intensity.numpy()\n",
    "pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "pcd\n",
    "\n",
    "# np.frombuffer(pcd.tobytes(), dtype='float32').reshape(-1, 4) # bytes에서 numpy로 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # npy to pcd\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_f = o3d.t.io.read_point_cloud('/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_0000/lidar/0000.pcd')\n",
    "\n",
    "positions = pcd_f.point.positions.numpy()\n",
    "intensity = pcd_f.point.intensity.numpy()\n",
    "pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "xyzi = np.stack([pcd[:,1]*-1, pcd[:, 0], pcd[:, 2], pcd[:, 3]], axis=1)\n",
    "\n",
    "xyz = xyzi[:, :3]\n",
    "i = [[i] for i in xyzi[:, 3]]\n",
    "\n",
    "pcd = o3d.t.geometry.PointCloud()\n",
    "pcd.point[\"positions\"] = o3d.core.Tensor(xyz)\n",
    "pcd.point[\"intensity\"] = o3d.core.Tensor(i)\n",
    "\n",
    "o3d.t.io.write_point_cloud('/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_0000/lidar/0011.pcd', pcd, write_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyzi = np.fromfile('/data/hwang/datasets/kitti/training/velodyne/000000.bin', dtype=np.float32).reshape(-1, 4)\n",
    "\n",
    "xyz = xyzi[:, :3]\n",
    "i = [[i] for i in xyzi[:, 3]]\n",
    "\n",
    "pcd = o3d.t.geometry.PointCloud()\n",
    "pcd.point[\"positions\"] = o3d.core.Tensor(xyz)\n",
    "pcd.point[\"intensity\"] = o3d.core.Tensor(i)\n",
    "\n",
    "o3d.t.io.write_point_cloud('/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_0000/lidar/0010.pcd', pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Fundamental Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python # cv2.xfeatures2d.sift_create()를 사용하기 위해서는 따로 설치 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 이미지 사이에 매칭되는 특징점을 최대한 많이 찾아내야 한다.\n",
    "# FLANN에 기반한 매처를 이용한 SIFT 디스크립터를 사용하고 ratio를 테스트한다.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1 = cv2.imread('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/Camera/CameraFront/blur/2-048_02244_CF_001.png')\n",
    "img2 = cv2.imread('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/Camera/CameraFront/blur/2-048_02244_CF_032.png')\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1, None)\n",
    "kp2, des2 = sift.detectAndCompute(img2, None)\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = {'algorithm': FLANN_INDEX_KDTREE, 'trees': 5}\n",
    "search_params = {'check': 50}\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, (m, n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        good.append(m)\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 이미지로부터 매칭되는 특징점 중 가장 좋은 것들을 이용해 Fundamental Matrix를 계산한다.\n",
    "\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_LMEDS)\n",
    "\n",
    "# We select only inlier points\n",
    "pts1 = pts1[mask.ravel()==1]\n",
    "pts2 = pts2[mask.ravel()==1]\n",
    "\n",
    "# Fundamental Matrix\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epiline 찾기\n",
    "\n",
    "def drawlines(img1, img2, lines, pts1, pts2):\n",
    "    '''img1 - image on which we draw epilines for the points in img2 \\n\n",
    "    lines - corresponding epilines'''    \n",
    "    \n",
    "    r, c, _ = img1.shape\n",
    "    # img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "    # img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    for r, pt1, pt2 in zip(lines, pts1, pts2):\n",
    "        color = tuple(np.random.randint(0,255,3).tolist())\n",
    "        x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0,y0), (x1,y1), color, 1)\n",
    "        img1 = cv2.circle(img1, tuple(pt1), 5, color, -1)\n",
    "        img2 = cv2.circle(img2, tuple(pt2), 5, color, -1)\n",
    "        \n",
    "    return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epiline을 찾아 그려준다.\n",
    "\n",
    "# Find epilines corresponding to points in right image (second image) and\n",
    "# drawing its lines on left image\n",
    "lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)\n",
    "lines1 = lines1.reshape(-1,3)\n",
    "img5,img6 = drawlines(img1,img2,lines1,pts1,pts2)\n",
    "# Find epilines corresponding to points in left image (first image) and\n",
    "# drawing its lines on right image\n",
    "lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)\n",
    "lines2 = lines2.reshape(-1,3)\n",
    "img3,img4 = drawlines(img2, img1, lines2, pts2, pts1)\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(121),plt.imshow(img5)\n",
    "plt.subplot(122),plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(F, 3)xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Essential Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, mask = cv2.findEssentialMat(points1=pts1, \n",
    "                               points2=pts2, \n",
    "                               cameraMatrix=intrinsic[:3, :3],\n",
    "                               method = cv2.RANSAC,\n",
    "                               prob = 0.999,\n",
    "                               threshold = 1.0)\n",
    "\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(np.matmul(E, F), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, r, t, _ = cv2.recoverPose(E, pts1, pts2, intrinsic[:3, :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.around(E, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 특수환경 자율주행 3D 이미지\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # make dataframe\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색용 코드\n",
    "df.loc[df['xyzlwh'].apply(lambda x: -31.16 in x[:3])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=['clip', 'frame', 'xyxy', 'xyzlwh', 'theta', 'eulerangle', 'translation', 'fxfycxcy', 'k1k2p1p2', 'pcd', 'class'])\n",
    "idx = 0\n",
    "\n",
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/training/'\n",
    "clips = sorted([i for i in os.listdir(path) if 'drive_' in i])\n",
    "# clip = clips[20]\n",
    "\n",
    "for clip in clips:\n",
    "    try:\n",
    "        with open(path+clip+'/calib.txt', 'r') as f:\n",
    "            calib = [re.sub('\\n', '', i) for i in f.readlines()]\n",
    "\n",
    "        eulerangle = calib[4].split(',')\n",
    "        translation = calib[6].split(',')\n",
    "        intrinsic = calib[8].split(',')\n",
    "        fxfycxcy = intrinsic[:4]\n",
    "        k1k2p1p2 = intrinsic[4:]\n",
    "\n",
    "        metas = sorted(os.listdir(path+clip+'/meta/'))\n",
    "\n",
    "        for meta in metas:\n",
    "            meta = path+clip+f'/meta/{meta}'\n",
    "            frame = meta[-11:-5]\n",
    "\n",
    "            pcd_f = o3d.t.io.read_point_cloud(path+clip+f'/lidar/{clip}_{frame}.pcd')\n",
    "            positions = pcd_f.point.positions.numpy()\n",
    "            intensity = pcd_f.point.intensity.numpy()\n",
    "            pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "            with open(meta, 'r') as f:\n",
    "                meta_js = json.load(f)\n",
    "\n",
    "            objects = meta_js['OBJECT_LIST'][0]['3D_LIST']\n",
    "            for i in range(len(objects)):\n",
    "                object = objects[i]\n",
    "                xyxy = object['BOX']\n",
    "                xyz = object['LOCATION']\n",
    "                hwl = object['DIMENSION']\n",
    "                xyzlwh = xyz + hwl[2:3] + hwl[1:2] + hwl[0:1]\n",
    "                # xyzlwh = ', '.join(map(str, xyzlwh))\n",
    "                theta = object['YAW']\n",
    "                class_ = object['TYPE']\n",
    "\n",
    "                train_df.loc[idx] = [clip, frame, xyxy, xyzlwh, theta, eulerangle, translation, fxfycxcy, k1k2p1p2, pcd, class_]\n",
    "\n",
    "                idx+=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    train_df.to_csv('/data/NIA50/data/특수환경 자율주행 3D 이미지/train_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # validation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(columns=['clip', 'frame', 'xyxy', 'xyzlwh', 'theta', 'eulerangle', 'translation', 'fxfycxcy', 'k1k2p1p2', 'class', 'type'])\n",
    "idx = 0\n",
    "\n",
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/'\n",
    "clips = sorted([i for i in os.listdir(path) if 'drive_' in i])\n",
    "# clip = clips[20]\n",
    "\n",
    "for clip in clips:\n",
    "    try:\n",
    "        with open(path+clip+'/calib.txt', 'r') as f:\n",
    "            calib = [re.sub('\\n', '', i) for i in f.readlines()]\n",
    "\n",
    "        eulerangle = list(map(float, calib[4].split(',')))\n",
    "        translation = list(map(float, calib[6].split(',')))\n",
    "        intrinsic = list(map(float, calib[8].split(',')))\n",
    "        fxfycxcy = intrinsic[:4]\n",
    "        k1k2p1p2 = intrinsic[4:]\n",
    "\n",
    "        metas = sorted(os.listdir(path+clip+'/lidar/'))\n",
    "\n",
    "        for meta in metas:\n",
    "            meta = path+clip+f'/meta/{meta}'\n",
    "            frame = meta[-11:-5]\n",
    "\n",
    "            # pcd_f = o3d.t.io.read_point_cloud(path+clip+f'/lidar/{clip}_{frame}.pcd')\n",
    "            # positions = pcd_f.point.positions.numpy()\n",
    "            # intensity = pcd_f.point.intensity.numpy()\n",
    "            # pcd = np.concatenate((positions, intensity), axis = 1).tobytes()\n",
    "\n",
    "            with open(meta, 'r') as f:\n",
    "                meta_js = json.load(f)\n",
    "\n",
    "            objects = meta_js['OBJECT_LIST'][0]['3D_LIST']\n",
    "            for i in range(len(objects)):\n",
    "                object = objects[i]\n",
    "                xyxy = object['BOX']\n",
    "                xyz = object['LOCATION']\n",
    "                hwl = object['DIMENSION']\n",
    "                xyzlwh = xyz + hwl[2:3] + hwl[1:2] + hwl[0:1]\n",
    "                # xyzlwh = ', '.join(map(str, xyzlwh))\n",
    "                theta = object['YAW']\n",
    "                class_ = object['CLASS']\n",
    "                type_ = object['TYPE']\n",
    "\n",
    "                val_df.loc[idx] = [clip, frame, xyxy, xyzlwh, theta, eulerangle, translation, fxfycxcy, k1k2p1p2, class_, type_]\n",
    "\n",
    "                idx+=1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    val_df.to_csv('/data/NIA50/data/특수환경 자율주행 3D 이미지/val_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # PV-RCNN (fit custom dataset)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # save npy\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('/data/NIA50/data/특수환경 자율주행 3D 이미지/val_df.csv')\n",
    "val_df['frame'] = val_df['frame'].apply(lambda x: str(x).zfill(6))\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/'\n",
    "save_path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/points'\n",
    "\n",
    "true_data = val_df.loc[val_df['xyzlwh']!='[0.0, 0.0, 0.0, 0, 0, 0]']\n",
    "clips = true_data['clip'].unique()\n",
    "\n",
    "for clip in clips:\n",
    "    frames = true_data.loc[true_data['clip'] == clip, 'frame'].unique()\n",
    "\n",
    "    for frame in frames:\n",
    "        try:\n",
    "            pcd_name = pcd_name = clip + f'_{frame}.pcd'\n",
    "            pcd_f = o3d.t.io.read_point_cloud(path+clip+'/lidar/'+pcd_name)\n",
    "\n",
    "            positions = pcd_f.point.positions.numpy()\n",
    "            intensity = pcd_f.point.intensity.numpy()\n",
    "            pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "            npy = np.save(save_path+f'/{clip}_{frame}.npy', pcd)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/ImageSets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # dividing train, val\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # imagesets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('/data/NIA50/data/특수환경 자율주행 3D 이미지/val_df.csv')\n",
    "val_df['frame'] = val_df['frame'].apply(lambda x: str(x).zfill(6))\n",
    "tdf = val_df.loc[val_df['xyzlwh']!='[0.0, 0.0, 0.0, 0, 0, 0]']\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "clips = tdf['clip'].unique()\n",
    "for clip in clips:\n",
    "\n",
    "    frames = tdf.loc[tdf['clip'] == clip, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "        image = clip+f'_{frame}'\n",
    "        images.append(image)\n",
    "\n",
    "len(images), len(os.listdir('/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/points'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sorted([re.sub('.npy', '', i) for i in os.listdir('/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/points')])\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/ImageSets/'\n",
    "\n",
    "train, val = train_test_split(images, test_size=0.2, random_state=0)\n",
    "\n",
    "with open(path+'train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(train)))\n",
    "\n",
    "with open(path+'val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # labels\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차집합\n",
    "\n",
    "set(labels) - set(images)\n",
    "set(labels).difference(set(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/labels/'\n",
    "labels = [re.sub('.txt', '', i) for i in os.listdir(path)]\n",
    "\n",
    "for n_pcd in set(labels) - set(images):\n",
    "    if os.path.exists(path+f'{n_pcd}.txt'):\n",
    "        os.remove(path+f'{n_pcd}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### # calibration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/calib/'\n",
    "\n",
    "clips = tdf['clip'].unique()\n",
    "for clip in clips:\n",
    "\n",
    "    frames = tdf.loc[tdf['clip'] == clip, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "        image = clip+f'_{frame}'\n",
    "\n",
    "        fxfycxcy = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'fxfycxcy'].values[0])\n",
    "        eulerangle = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'eulerangle'].values[0])\n",
    "        translation = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'translation'].values[0])\n",
    "        intrinsic = np.asarray([[fxfycxcy[0], 0, fxfycxcy[2]], [0, fxfycxcy[1], fxfycxcy[3]], [0, 0, 1]])\n",
    "        rotation_matrix = R.from_euler('xyz', eulerangle, degrees=True).as_matrix()\n",
    "\n",
    "        p0 = np.hstack([intrinsic, np.zeros((3, 1))])\n",
    "        R0_rect = np.eye(3)\n",
    "        Tr_velo_to_cam = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "        Tr_imu_to_velo = np.zeros((12))\n",
    "\n",
    "        # to string\n",
    "        p0 = ' '.join(map(str, p0.reshape(-1).tolist()))\n",
    "        p1 = p0\n",
    "        p2 = p0\n",
    "        p3 = p0\n",
    "        R0_rect = ' '.join(map(str, R0_rect.reshape(-1).tolist()))\n",
    "        Tr_velo_to_cam = ' '.join(map(str, Tr_velo_to_cam.reshape(-1).tolist()))\n",
    "        Tr_imu_to_velo = ' '.join(map(str, Tr_imu_to_velo.reshape(-1).tolist()))\n",
    "\n",
    "        label_lines = ['P0: '+p0, 'P1: '+p1, 'P2: '+p2, 'P3: '+p3, 'R0_rect: '+R0_rect, 'Tr_velo_to_cam: '+Tr_velo_to_cam, 'Tr_imu_to_velo: '+Tr_imu_to_velo]\n",
    "        with open(path + f'{image}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(label_lines))\n",
    "\n",
    "            # f.write(f'P0: {p0}\\n')\n",
    "            # f.write(f'P1: {p1}\\n')\n",
    "            # f.write(f'P2: {p2}\\n')\n",
    "            # f.write(f'P3: {p3}\\n')\n",
    "            # f.write(f'R0_rect: {R0_rect}\\n')\n",
    "            # f.write(f'Tr_velo_to_cam: {Tr_velo_to_cam}\\n')\n",
    "            # f.write(f'Tr_imu_to_velo: {Tr_imu_to_velo}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/calib/'\n",
    "labels = [re.sub('.txt', '', i) for i in os.listdir(path)]\n",
    "\n",
    "for n_pcd in set(labels) - set(images):\n",
    "    if os.path.exists(path+f'{n_pcd}.txt'):\n",
    "        os.remove(path+f'{n_pcd}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # PointRCNN (fit kitti datset)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv('/data/NIA50/data/특수환경 자율주행 3D 이미지/val_df.csv')\n",
    "val_df['frame'] = val_df['frame'].apply(lambda x: str(x).zfill(6))\n",
    "images = []\n",
    "for i in val_df.index.values:\n",
    "    image = val_df.loc[i]['clip'] + '_' + val_df.loc[i]['frame']\n",
    "    images.append(image)\n",
    "val_df['image'] = images\n",
    "tdf = val_df.loc[val_df['xyzlwh']!='[0.0, 0.0, 0.0, 0, 0, 0]']\n",
    "tdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # imagesets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 복사\n",
    "\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "exi_pcd = glob.glob('/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/*/lidar/*.pcd')\n",
    "exi_pcd = [i[-21:-4] for i in exi_pcd]\n",
    "\n",
    "images_true = []\n",
    "for clip in tdf['clip'].unique():\n",
    "    for frame in tdf.loc[tdf['clip'] == clip, 'frame'].unique():\n",
    "        image = clip + f'_{frame}'\n",
    "        images_true.append(image)\n",
    "        \n",
    "images = sorted(set(images_true) & set(exi_pcd))\n",
    "# images = sorted([re.sub('.npy', '', i) for i in os.listdir('/data/NIA50/data/특수환경 자율주행 3D 이미지/mm_Train3D/points')])\n",
    "\n",
    "images_re, test = train_test_split(images, test_size = 0.1, random_state = 0)\n",
    "train, val = train_test_split(images_re, test_size = 0.2, random_state = 0)\n",
    "\n",
    "path = '/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/imagesets/'\n",
    "\n",
    "# with open(path+'train.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(train)))\n",
    "\n",
    "# with open(path+'val.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(val)))\n",
    "    \n",
    "# with open(path+'test.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train), len(val), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # velodyne\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_files = glob.glob('/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/*/lidar/*.pcd')\n",
    "\n",
    "pcd_path = []\n",
    "for pcd_file in pcd_files:\n",
    "    for image in images:\n",
    "        if image == pcd_file[-21:-4]:\n",
    "            pcd_path.append(pcd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/velodyne', exist_ok=True)\n",
    "os.makedirs('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/testing/velodyne', exist_ok=True)\n",
    "\n",
    "for pcd_file in pcd_path:\n",
    "    pcd_f = o3d.t.io.read_point_cloud(pcd_file)\n",
    "    positions = pcd_f.point.positions.numpy()\n",
    "    intensity = pcd_f.point.intensity.numpy()\n",
    "    \n",
    "    pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "    pcd_bytes = pcd.tobytes()\n",
    "    \n",
    "    if pcd_file[-21:-4] in train:\n",
    "        with open(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/velodyne/{pcd_file[-21:-4]}.bin', 'wb') as f:\n",
    "            f.write(pcd_bytes)\n",
    "            \n",
    "    elif pcd_file[-21:-4] in val:\n",
    "        with open(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/velodyne/{pcd_file[-21:-4]}.bin', 'wb') as f:\n",
    "            f.write(pcd_bytes)\n",
    "\n",
    "    elif pcd_file[-21:-4] in test:\n",
    "        with open(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/testing/velodyne/{pcd_file[-21:-4]}.bin', 'wb') as f:\n",
    "            f.write(pcd_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # calib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([[0, 1, 0],\n",
    "            [0, 0, 1],\n",
    "            [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "\n",
    "clips = tdf['clip'].unique()\n",
    "for clip in clips[:1]:\n",
    "\n",
    "    frames = tdf.loc[tdf['clip'] == clip, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "        image = clip+f'_{frame}'\n",
    "\n",
    "        fxfycxcy = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'fxfycxcy'].values[0])\n",
    "        # eulerangle = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'eulerangle'].values[0]) * np.asarray(180/np.pi)\n",
    "        eulerangle = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'eulerangle'].values[0])\n",
    "        translation = json.loads(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'translation'].values[0])\n",
    "        intrinsic = np.asarray([[fxfycxcy[0], 0, fxfycxcy[2]], [0, fxfycxcy[1], fxfycxcy[3]], [0, 0, 1]])\n",
    "        rotation_matrix = R.from_euler('xyz', eulerangle, degrees=True).as_matrix()\n",
    "\n",
    "        p0 = np.hstack([intrinsic, np.zeros((3, 1))])\n",
    "        R0_rect = np.eye(3)\n",
    "        # velo_to_cam_rotation = np.asarray([[0, 1, 0],\n",
    "        #                                    [0, 0, 1],\n",
    "        #                                    [1, 0, 0]])\n",
    "        Tr_velo_to_cam = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "        Tr_imu_to_velo = np.zeros((12))\n",
    "\n",
    "        # to string\n",
    "        p0 = ' '.join(map(str, p0.reshape(-1).tolist()))\n",
    "        p1 = p0\n",
    "        p2 = p0\n",
    "        p3 = p0\n",
    "        R0_rect = ' '.join(map(str, R0_rect.reshape(-1).tolist()))\n",
    "        # R0_rect = ' '.join(map(str, rotation_matrix.reshape(-1).tolist()))\n",
    "        Tr_velo_to_cam = ' '.join(map(str, Tr_velo_to_cam.reshape(-1).tolist()))\n",
    "        Tr_imu_to_velo = ' '.join(map(str, Tr_imu_to_velo.reshape(-1).tolist()))\n",
    "\n",
    "        label_lines = ['P0: '+p0, 'P1: '+p1, 'P2: '+p2, 'P3: '+p3, 'R0_rect: '+R0_rect, 'Tr_velo_to_cam: '+Tr_velo_to_cam, 'Tr_imu_to_velo: '+Tr_imu_to_velo]\n",
    "                \n",
    "        # if image in train:\n",
    "        #     with open('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/calib/' + f'{image}.txt', 'w') as f:\n",
    "        #         f.write('\\n'.join(label_lines))\n",
    "                \n",
    "        if image in sorted(val)[:1]:\n",
    "            with open('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/calib/' + f'{image}.txt', 'w') as f:\n",
    "                f.write('\\n'.join(label_lines))\n",
    "                \n",
    "        # elif image in test:\n",
    "        #     with open('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/testing/calib/' + f'{image}.txt', 'w') as f:\n",
    "        #         f.write('\\n'.join(label_lines))\n",
    "                \n",
    "            # f.write(f'P0: {p0}\\n')\n",
    "            # f.write(f'P1: {p1}\\n')\n",
    "            # f.write(f'P2: {p2}\\n')\n",
    "            # f.write(f'P3: {p3}\\n')\n",
    "            # f.write(f'R0_rect: {R0_rect}\\n')\n",
    "            # f.write(f'Tr_velo_to_cam: {Tr_velo_to_cam}\\n')\n",
    "            # f.write(f'Tr_imu_to_velo: {Tr_imu_to_velo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # label_2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha 구하는 공식\n",
    "\n",
    "import math\n",
    "\n",
    "def normalizeAngle(angle):\n",
    "    result = angle % (2*math.pi)\n",
    "    if result < -math.pi:\n",
    "        result += 2*math.pi\n",
    "    elif result > math.pi:\n",
    "        result -= 2*math.pi\n",
    "    return result\n",
    "\n",
    "def cal_alpha_ori(x, z, ry):  \n",
    "    angle = ry\n",
    "    angle -= -math.atan2(z, x) -1.5*math.pi \n",
    "    alpha = normalizeAngle(angle)\n",
    "    return alpha # -1.818032754845337\n",
    "\n",
    "cal_alpha_ori(2.5702, 9.7190, -1.5595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/label_2', exist_ok=True)\n",
    "\n",
    "idx = tdf.index.values\n",
    "\n",
    "for i in idx[:1]:\n",
    "    clip = tdf.loc[i]['clip']\n",
    "    frame = tdf.loc[i]['frame']\n",
    "    image = clip + f'_{frame}'\n",
    "    \n",
    "    type_ = tdf.loc[i]['class']\n",
    "    truncation = 0.00\n",
    "    occulsion = 0\n",
    "    xyxy = json.loads(tdf.loc[i]['xyxy'])\n",
    "    xyzlwh = json.loads(tdf.loc[i]['xyzlwh'])\n",
    "    hwlxyz = [xyzlwh[5], xyzlwh[4], xyzlwh[3], xyzlwh[0], xyzlwh[1], xyzlwh[2]]\n",
    "    rotation_y = tdf.loc[i]['theta'] # rotation_y = theta\n",
    "    alpha = np.around(cal_alpha_ori(xyzlwh[0]*-1, xyzlwh[2], rotation_y), 2)\n",
    "    \n",
    "    if type_ == 'VEHICLE': type_ = 'Car'\n",
    "    elif type_ == 'BICYCLE': type_ = 'Cyclist'\n",
    "    elif type_ == 'PEDESTRIAN': type_ = 'Pedestrian'\n",
    "    \n",
    "    if xyxy[0] < 0: xyxy[0] = 0\n",
    "    if xyxy[1] < 0: xyxy[1] = 0\n",
    "    if xyxy[2] > 1920: xyxy[2] = 1920\n",
    "    if xyxy[3] > 1200: xyxy[3] = 1200\n",
    "    \n",
    "    xyxy = ' '.join(map(str, xyxy))\n",
    "    hwlxyz = ' '.join(map(str, hwlxyz))\n",
    "    \n",
    "    label = list(map(str, [type_, truncation, occulsion, alpha, xyxy, hwlxyz, rotation_y]))\n",
    "    \n",
    "    if image in train + val:\n",
    "        with open(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/label_2/{image}.txt', 'w') as f:\n",
    "            f.write(' '.join(label))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clips = tdf['clip'].unique()\n",
    "# for clip in clips[:1]:\n",
    "    \n",
    "#     frames = tdf.loc[tdf['clip']==clip, 'frame'].unique()\n",
    "#     for frame in frames:\n",
    "        \n",
    "#         image = clip+f'_{frame}'\n",
    "#         for idx in len(tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame))\n",
    "#         class_ = tdf.loc[(tdf['clip']==clip) & (tdf['frame']==frame), 'class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # image_2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/image_2', exist_ok=True)\n",
    "os.makedirs('/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/testing/image_2', exist_ok=True)\n",
    "\n",
    "for train_img in train + val:\n",
    "    copy_path = glob.glob(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/*/image_0/{train_img}.jpg')[0]\n",
    "    paste_path = f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/training/image_2/{train_img}.jpg'\n",
    "    shutil.copy(copy_path, paste_path)\n",
    "    \n",
    "# for test_img in test:\n",
    "#     copy_path = glob.glob(f'/data/NIA50/data/특수환경 자율주행 3D 이미지/Validation/*/image_0/{test_img}.jpg')[0]\n",
    "#     paste_path = f'/data/NIA50/data/특수환경 자율주행 3D 이미지/pointrcnn_Train3D/testing/image_2/{test_img}.jpg'\n",
    "#     shutil.copy(copy_path, paste_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # NIA48\n",
    "---\n",
    "- PointRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_and_resized_intrinsic(\n",
    "    fx, fy, cx, cy, crop_cx, crop_cy, resize_fx, resize_fy):\n",
    "    '''\n",
    "    crop_cx : crop size of u axis orientation in image plane\n",
    "    crop_cy : crop size of v axis orientation in image plane\n",
    "    resize_fx : resize ratio of width orientation in image plane\n",
    "    resize_fy : resize ratio of height orientation in image plane    \n",
    "    '''\n",
    "\n",
    "    cx -= crop_cx\n",
    "    cy -= crop_cy\n",
    "\n",
    "    fx *= resize_fx\n",
    "    fy *= resize_fy\n",
    "\n",
    "    cx *= resize_fx\n",
    "    cy *= resize_fy\n",
    "\n",
    "    return fx, fy, cx, cy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 숫자리스트로 바꾸는 함수\n",
    "def str_cng(txt):\n",
    "    txt = txt.replace('\\n', '').split(',')\n",
    "    txt = list(map(float, txt))\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # calib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/data/NIA50/50-2/data/NIA48/training/calib/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/calib/Lidar_camera_calib/2-048_02244_LCC_CF.txt') as f:\n",
    "    camera_calib = f.readlines()\n",
    "# with open('/data/NIA48/S_Clip_03400_06/calib/Lidar_radar_calib/2-048_03400_LRC_RF.txt') as f:\n",
    "#     radar_calib = f.readlines()\n",
    "\n",
    "essential_matrix = np.asarray([[0, 1, 0],\n",
    "                               [0, 0, 1],\n",
    "                               [1, 0, 0]])\n",
    "# essential_matrix = E\n",
    "eulerangle = str2list(camera_calib[4]) # z, x, y\n",
    "# eulerangle = np.asarray(eulerangle) + np.asarray([-360*np.pi/180, -90*np.pi/180, -90*np.pi/180])\n",
    "X = R.from_euler('X', eulerangle[1]).as_matrix()\n",
    "Y = R.from_euler('Y', eulerangle[0]).as_matrix()\n",
    "Z = R.from_euler('Z', eulerangle[2]).as_matrix()\n",
    "rotation_matrix = Z @ Y @ X\n",
    "# rotation_matrix = R.from_euler('xyz', eulerangle).as_matrix()\n",
    "# e_rotation_matrix = np.matmul(rotation_matrix, essential_matrix)\n",
    "translation = np.asarray(str2list(camera_calib[6]))\n",
    "fpd = str2list(camera_calib[8])\n",
    "# resize_fpd = get_cropped_and_resized_intrinsic(fpd[0], fpd[1], fpd[2], fpd[3], 0, 0, 1920/735, 1200/575)\n",
    "# fpd = resize_fpd\n",
    "intrinsic = np.asarray([fpd[0], 0, fpd[2], 0,\n",
    "                        0, fpd[1], fpd[3], 0,\n",
    "                        0, 0, 1, 0]).reshape(3,4)\n",
    "distcoeffs = str2list(camera_calib[8])[4:]\n",
    "# new_intrinsic, _ = cv2.getOptimalNewCameraMatrix(cameraMatrix=intrinsic[:3, :3],\n",
    "#                                                 distCoeffs=np.asarray(distcoeffs),\n",
    "#                                                 imageSize=(1200, 1920),\n",
    "#                                                 alpha=1)\n",
    "# intrinsic = np.hstack([new_intrinsic, np.asarray([0, 0, 0]).reshape(3, 1)])\n",
    "extrinsic = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "# extrinsic = np.hstack([e_rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "\n",
    "# projection_matrix = intrinsic\n",
    "# projection_matrix = np.matmul(intrinsic, extrinsic)\n",
    "# projection_matrix[2, 2] = 1\n",
    "# projection_matrix[:, 3] = 0\n",
    "\n",
    "# p2 = [projection_matrix[0, 0], 0, projection_matrix[0, 2], 0,\n",
    "#       0, projection_matrix[1, 1], projection_matrix[1, 2], 0,\n",
    "#       0, 0, 1, 0]\n",
    "# p2 = projection_matrix.reshape(-1).tolist()\n",
    "p2 = intrinsic.reshape(-1).tolist()\n",
    "\n",
    "R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "# R0_rect = rotation_matrix.reshape(-1).tolist()\n",
    "Tr_velo_to_cam = extrinsic.reshape(-1).tolist()\n",
    "# Tr_velo_to_cam = np.zeros((12)).tolist()\n",
    "Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "calib_kitti =  ['P0: '+list2str(p2), \n",
    "                'P1: '+list2str(p2), \n",
    "                'P2: '+list2str(p2), \n",
    "                'P3: '+list2str(p2), \n",
    "                'R0_rect: '+list2str(R0_rect), \n",
    "                'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "                'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "with open(path + 'test48.txt', 'w') as f:\n",
    "    f.write('\\n'.join(calib_kitti))\n",
    "\n",
    "# print(essential_matrix, '\\n')\n",
    "# print(rotation_matrix, '\\n')\n",
    "# print(extrinsic, '\\n')\n",
    "\n",
    "# with open('/data/hwang/datasets/kitti/training/calib/000038.txt') as f:\n",
    "#     calib = f.readlines()\n",
    "#     extrinsic_ = list(map(float, calib[5].replace('\\n', '').split(' ')[1:]))\n",
    "#     extrinsic_ = np.asarray(extrinsic_).reshape(3, 4)    \n",
    "#     R0_rect = list(map(float, calib[4].replace('\\n', '').split(' ')[1:]))\n",
    "#     R0_rect = np.asarray(R0_rect).reshape(3, 3)\n",
    "\n",
    "# print(extrinsic_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # label_2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/50-2/data/NIA48/training/label_2/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "with open('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/result/2-048_02244_FC_001.json', 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "type_ = meta['annotation'][0]['category']\n",
    "truncation = 0.0\n",
    "occulsion = 0\n",
    "xyxy = meta['annotation'][0]['3d_box'][0]['2d_box']\n",
    "xyz = meta['annotation'][0]['3d_box'][0]['location']\n",
    "rotation_y = abs(meta['annotation'][0]['3d_box'][0]['rotation_y']) - 90 * np.pi/180\n",
    "\n",
    "xyz_re = np.matmul(extrinsic, np.asarray(xyz + [1]).reshape(4, 1)).reshape(-1).tolist()\n",
    "# xyz = np.matmul(e_rotation_matrix, np.asarray(xyz).reshape(3, 1)).reshape(-1).tolist()\n",
    "whl = meta['annotation'][0]['3d_box'][0]['dimension']\n",
    "hwlxyz = [whl[1], whl[0], whl[2]] + xyz_re\n",
    "\n",
    "alpha = cal_alpha_ori(xyz_re[0], xyz_re[2], rotation_y)\n",
    "\n",
    "xyxy = ' '.join(map(str, xyxy))\n",
    "hwlxyz = ' '.join(map(str, hwlxyz))\n",
    "\n",
    "label = list(map(str, [type_, truncation, occulsion, alpha, xyxy, hwlxyz, rotation_y]))\n",
    "\n",
    "with open(path+'test48.txt', 'w') as f:\n",
    "    f.write(' '.join(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = '02244'\n",
    "\n",
    "path = '/data/NIA50/50-2/data/NIA48/training/calib/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open(f'/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_{num}_02/calib/Lidar_camera_calib/2-048_{num}_LCC_CF.txt') as f:\n",
    "    camera_calib = f.readlines()\n",
    "    \n",
    "eulerangle = str2list(camera_calib[4])\n",
    "eulerangle = np.asarray(eulerangle) + np.asarray([90*np.pi/180, -90*np.pi/180, -180*np.pi/180])\n",
    "eulerangle = np.asarray(eulerangle) + np.asarray([0*np.pi/180, 0*np.pi/180, 90*np.pi/180])\n",
    "# eulerangle[2] = eulerangle[2]*-1\n",
    "\n",
    "X = R.from_euler('X', eulerangle[1]).as_matrix()\n",
    "Y = R.from_euler('Y', eulerangle[0]).as_matrix()\n",
    "Z = R.from_euler('Z', eulerangle[2]).as_matrix()\n",
    "rotation_matrix = Z @ Y @ X\n",
    "\n",
    "# e_rotation_matrix = np.matmul(essential_matrix, rotation_matrix)\n",
    "# e_rotation_matrix = np.matmul(rotation_matrix, essential_matrix)\n",
    "translation = np.asarray(str2list(camera_calib[6]))\n",
    "# translation = translation = np.asarray(str2list(camera_calib[6])) + np.asarray([0, 0.6, 0.2])\n",
    "fpd = str2list(camera_calib[8])\n",
    "# resize_fpd = get_cropped_and_resized_intrinsic(fpd[0], fpd[1], fpd[2], fpd[3], 0, 0, 1920/745, 1200/575)\n",
    "# fpd = resize_fpd\n",
    "intrinsic = np.asarray([fpd[0], 0, fpd[2], 0,\n",
    "                        0, fpd[1], fpd[3], 0,\n",
    "                        0, 0, 1, 0]).reshape(3,4)\n",
    "# distortion = str2list(camera_calib[8])[4:]\n",
    "# new_intrinsic, _ = cv2.getOptimalNewCameraMatrix(cameraMatrix=intrinsic[:3, :3],\n",
    "#                                                 distCoeffs=np.asarray(distortion),\n",
    "#                                                 imageSize=(1200, 1920),\n",
    "#                                                 alpha=1)\n",
    "# intrinsic = np.hstack([new_intrinsic, np.asarray([0, 0, 0]).reshape(3, 1)])\n",
    "extrinsic = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "\n",
    "p2 = intrinsic.reshape(-1).tolist()\n",
    "\n",
    "R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "# R0_rect = rotation_matrix.reshape(-1).tolist()\n",
    "Tr_velo_to_cam = extrinsic.reshape(-1).tolist()\n",
    "# Tr_velo_to_cam = np.zeros((12)).tolist()\n",
    "Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "calib_kitti =  ['P0: '+list2str(p2), \n",
    "                'P1: '+list2str(p2), \n",
    "                'P2: '+list2str(p2), \n",
    "                'P3: '+list2str(p2), \n",
    "                'R0_rect: '+list2str(R0_rect), \n",
    "                'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "                'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "with open(path + 'test48.txt', 'w') as f:\n",
    "    f.write('\\n'.join(calib_kitti))\n",
    "\n",
    "path = '/data/NIA50/50-2/data/NIA48/training/label_2/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(f'/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_{num}_02/result/2-048_{num}_FC_001.json', 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "type_ = meta['annotation'][0]['category']\n",
    "truncation = 0.0\n",
    "occulsion = 0\n",
    "xyxy = meta['annotation'][0]['3d_box'][0]['2d_box']\n",
    "xyz = meta['annotation'][0]['3d_box'][0]['location']\n",
    "rotation_y = np.abs(meta['annotation'][0]['3d_box'][0]['rotation_y']) - 90 * np.pi/180 # radian\n",
    "# rotation_y = meta['annotation'][0]['3d_box'][0]['rotation_y']\n",
    "\n",
    "\n",
    "xyz_re = np.matmul(extrinsic, np.asarray(xyz + [1]).reshape(4, 1)).reshape(-1).tolist()\n",
    "# xyz = np.matmul(e_rotation_matrix, np.asarray(xyz).reshape(3, 1)).reshape(-1).tolist()\n",
    "whl = meta['annotation'][0]['3d_box'][0]['dimension']\n",
    "hwlxyz = [whl[1], whl[0], whl[2]] + xyz_re\n",
    "# hwlxyz = [whl[1], whl[0], whl[2]] + [xyz_re[2], xyz_re[1], xyz_re[0]]\n",
    "\n",
    "alpha = cal_alpha_ori(xyz_re[0], xyz_re[2], rotation_y)\n",
    "\n",
    "xyxy = ' '.join(map(str, xyxy))\n",
    "hwlxyz = ' '.join(map(str, hwlxyz))\n",
    "\n",
    "label = list(map(str, [type_, truncation, occulsion, alpha, xyxy, hwlxyz, rotation_y]))\n",
    "\n",
    "with open(path+'test48.txt', 'w') as f:\n",
    "    f.write(' '.join(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eulerangle = str2list(camera_calib[4])\n",
    "# eulerangle = np.asarray(eulerangle) + np.asarray([-180*np.pi/180, 90*np.pi/180, -90*np.pi/180])\n",
    "R.from_euler('XYZ', eulerangle).as_matrix().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eulerangle = str2list(camera_calib[4])\n",
    "# eulerangle = np.asarray(eulerangle) + np.asarray([-180*np.pi/180, 90*np.pi/180, -90*np.pi/180])\n",
    "R.from_euler('xyz', eulerangle).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(Tr_velo, dtype=float).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/data/NIA50/50-2/data/NIA48/training/calib/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "with open('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/calib/Lidar_camera_calib/2-048_02244_LCC_CF.txt') as f:\n",
    "    camera_calib = f.readlines()\n",
    "# with open('/data/NIA48/S_Clip_03400_06/calib/Lidar_radar_calib/2-048_03400_LRC_RF.txt') as f:\n",
    "#     radar_calib = f.readlines()\n",
    "\n",
    "# essential_matrix = np.asarray([[0, 1, 0],\n",
    "#                                [0, 0, 1],\n",
    "#                                [1, 0, 0]])\n",
    "# essential_matrix = E\n",
    "eulerangle = str2list(camera_calib[4])\n",
    "eulerangle = np.asarray(eulerangle) + np.asarray([360*np.pi/180, 90*np.pi/180, 90*np.pi/180])\n",
    "rotation_matrix = R.from_euler('xyz', eulerangle).as_matrix()\n",
    "# e_rotation_matrix = np.matmul(essential_matrix, rotation_matrix)\n",
    "# e_rotation_matrix = np.matmul(rotation_matrix, essential_matrix)\n",
    "translation = np.asarray(str2list(camera_calib[6]))\n",
    "fpd = str2list(camera_calib[8])\n",
    "intrinsic = np.asarray([fpd[0], 0, fpd[2], 0,\n",
    "                        0, fpd[1], fpd[3], 0,\n",
    "                        0, 0, 1, 0]).reshape(3,4)\n",
    "distortion = str2list(camera_calib[8])[4:]\n",
    "new_intrinsic, _ = cv2.getOptimalNewCameraMatrix(cameraMatrix=intrinsic[:3, :3],\n",
    "                                                distCoeffs=np.asarray(distortion),\n",
    "                                                imageSize=(1200, 1920),\n",
    "                                                alpha=0)\n",
    "intrinsic = np.hstack([new_intrinsic, np.asarray([0, 0, 0]).reshape(3, 1)])\n",
    "extrinsic = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "# extrinsic = np.hstack([e_rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "\n",
    "# projection_matrix = intrinsic\n",
    "# projection_matrix = np.matmul(intrinsic, extrinsic)\n",
    "# projection_matrix[2, 2] = 1\n",
    "# projection_matrix[:, 3] = 0\n",
    "\n",
    "# p2 = [projection_matrix[0, 0], 0, projection_matrix[0, 2], 0,\n",
    "#       0, projection_matrix[1, 1], projection_matrix[1, 2], 0,\n",
    "#       0, 0, 1, 0]\n",
    "# p2 = projection_matrix.reshape(-1).tolist()\n",
    "p2 = intrinsic.reshape(-1).tolist()\n",
    "\n",
    "R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "# R0_rect = rotation_matrix.reshape(-1).tolist()\n",
    "Tr_velo_to_cam = extrinsic.reshape(-1).tolist()\n",
    "# Tr_velo_to_cam = np.zeros((12)).tolist()\n",
    "Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "calib_kitti =  ['P0: '+list2str(p2), \n",
    "                'P1: '+list2str(p2), \n",
    "                'P2: '+list2str(p2), \n",
    "                'P3: '+list2str(p2), \n",
    "                'R0_rect: '+list2str(R0_rect), \n",
    "                'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "                'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "with open(path + 'test48.txt', 'w') as f:\n",
    "    f.write('\\n'.join(calib_kitti))\n",
    "\n",
    "# print(essential_matrix, '\\n')\n",
    "# print(rotation_matrix, '\\n')\n",
    "# print(extrinsic, '\\n')\n",
    "\n",
    "# with open('/data/hwang/datasets/kitti/training/calib/000038.txt') as f:\n",
    "#     calib = f.readlines()\n",
    "#     extrinsic_ = list(map(float, calib[5].replace('\\n', '').split(' ')[1:]))\n",
    "#     extrinsic_ = np.asarray(extrinsic_).reshape(3, 4)    \n",
    "#     R0_rect = list(map(float, calib[4].replace('\\n', '').split(' ')[1:]))\n",
    "#     R0_rect = np.asarray(R0_rect).reshape(3, 3)\n",
    "\n",
    "# print(extrinsic_)\n",
    "\n",
    "path = '/data/NIA50/50-2/data/NIA48/training/label_2/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "with open('/data/NIA50/50-2/data/NIA48/fine_data/S_Clip_02244_02/result/2-048_02244_FC_001.json', 'r') as f:\n",
    "    meta = json.load(f)\n",
    "    \n",
    "type_ = meta['annotation'][0]['category']\n",
    "truncation = 0.0\n",
    "occulsion = 0\n",
    "xyxy = meta['annotation'][0]['3d_box'][0]['2d_box']\n",
    "xyz = meta['annotation'][0]['3d_box'][0]['location']\n",
    "rotation_y = abs(meta['annotation'][0]['3d_box'][0]['rotation_y']) - 90 * np.pi/180\n",
    "\n",
    "xyz_re = np.matmul(extrinsic, np.asarray(xyz + [1]).reshape(4, 1)).reshape(-1).tolist()\n",
    "# xyz = np.matmul(e_rotation_matrix, np.asarray(xyz).reshape(3, 1)).reshape(-1).tolist()\n",
    "whl = meta['annotation'][0]['3d_box'][0]['dimension']\n",
    "hwlxyz = [whl[1], whl[0], whl[2]] + xyz_re\n",
    "\n",
    "alpha = cal_alpha_ori(xyz_re[0], xyz_re[2], rotation_y)\n",
    "\n",
    "xyxy = ' '.join(map(str, xyxy))\n",
    "hwlxyz = ' '.join(map(str, hwlxyz))\n",
    "\n",
    "label = list(map(str, [type_, truncation, occulsion, alpha, xyxy, hwlxyz, rotation_y]))\n",
    "\n",
    "with open(path+'test48.txt', 'w') as f:\n",
    "    f.write(' '.join(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # NIA50\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # DeepfusionMOT\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "\n",
    "calibs = sorted(glob.glob('/data/NIA50/50-2/data/NIA50/nia50_all/raw/*/calib/camera/camera_0.json'))\n",
    "\n",
    "calib_ls = []\n",
    "scenes = []\n",
    "for calib in calibs:\n",
    "    scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    if calib['extrinsic'] not in calib_ls:\n",
    "        calib_ls.append(calib['extrinsic'])\n",
    "        scenes.append(scene)\n",
    "    \n",
    "calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "             'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "             'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "             'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov5, pvrcnn inference 완료한 데이터\n",
    "\n",
    "with open('/data/NIA50/50-2/data/NIA50/nia50_all/pvrcnn_allcat/ImageSets/test.txt', 'r') as f:\n",
    "    scenes = [re.sub('\\n', '', i)[:-5] for i in f.readlines()]\n",
    "    scenes = sorted(list(set(scenes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '/data/NIA50/50-2/data/NIA50/nia50_all/raw/'\n",
    "save_path = '/data/NIA50/50-2/data/NIA50/nia50_all/deepfusionmot_allcat/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # calib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_calib = f'{save_path}/calib/'\n",
    "os.makedirs(save_calib, exist_ok=True)\n",
    "\n",
    "for scene in scenes:\n",
    "    calib = sorted(glob.glob(f'{source_path}/{scene}/calib/camera/*.json'))[0]\n",
    "\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "\n",
    "    for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "        if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "            extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "            extrinsic[:3, 3] -= extrinsic[:3, 2] * calib_typ[typ]['mov_zpoint']\n",
    "\n",
    "    p2 = np.hstack([np.asarray(calib['intrinsic']).reshape(3, 3), np.asarray([0, 0, 0]).reshape(3, -1)]).flatten().tolist()\n",
    "    R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "    Tr_velo_to_cam = extrinsic.flatten().tolist()[:12]\n",
    "    Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "    calib_kitti =  ['P0: '+list2str(p2), \n",
    "                    'P1: '+list2str(p2), \n",
    "                    'P2: '+list2str(p2), \n",
    "                    'P3: '+list2str(p2), \n",
    "                    'R0_rect: '+list2str(R0_rect), \n",
    "                    'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "                    'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "    with open(save_calib+f'{scene}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(calib_kitti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_calib = f'{save_path}/calib/'\n",
    "# os.makedirs(save_calib, exist_ok=True)\n",
    "\n",
    "# for scene in scenes:\n",
    "#     calib = sorted(glob.glob(f'{source_path}/{scene}/calib/camera/*.json'))[0]\n",
    "# #  = re.search('Suwon_[a-zA-Z0-9_]+', calib).group()\n",
    "# # # img_path = '/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_2210261635_0001_calib_ok/camera/camera_0'\n",
    "# # # frame = re.search('[0-9]+.pcd', calib).group().replace('.json', '')\n",
    "\n",
    "#     with open(calib, 'r') as f:\n",
    "#         calib = json.load(f)\n",
    "\n",
    "#     p2 = np.hstack([np.asarray(calib['intrinsic']).reshape(3, 3), np.asarray([0, 0, 0]).reshape(3, -1)]).flatten().tolist()\n",
    "#     R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "#     Tr_velo_to_cam = calib['extrinsic'][:12]\n",
    "#     Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "#     calib_kitti =  ['P0: '+list2str(p2), \n",
    "#                     'P1: '+list2str(p2), \n",
    "#                     'P2: '+list2str(p2), \n",
    "#                     'P3: '+list2str(p2), \n",
    "#                     'R0_rect: '+list2str(R0_rect), \n",
    "#                     'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "#                     'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "#     with open(save_calib+f'{scene}.txt', 'w') as f:\n",
    "#         f.write('\\n'.join(calib_kitti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/calib', exist_ok=True)\n",
    "\n",
    "\n",
    "# path = '/data/NIA50/50-2/data/NIA50_samples/'\n",
    "\n",
    "# clips = sorted(os.listdir(path))\n",
    "# for clip in clips[1:2]:\n",
    "#     frames = sorted(os.listdir(path + clip + '/lidar'))\n",
    "    \n",
    "#     for frame in frames:\n",
    "#         frame = frame[:4]\n",
    "#         with open(path+f'{clip}/calib_1.txt') as f:\n",
    "#             calib = f.readlines()\n",
    "\n",
    "#         # essential_matrix = np.asarray([[0, 1, 0],\n",
    "#         #                                [0, 0, 1],\n",
    "#         #                                [1, 0, 0]])\n",
    "#         # essential_matrix = np.eye(3)\n",
    "#         eulerangle = str2list(calib[4])\n",
    "#         X = R.from_euler('X', eulerangle[1]).as_matrix()\n",
    "#         Y = R.from_euler('Y', eulerangle[0]).as_matrix()\n",
    "#         Z = R.from_euler('Z', eulerangle[2]).as_matrix()\n",
    "#         # X = R.from_euler('X', eulerangle[2]).as_matrix()\n",
    "#         # Y = R.from_euler('Y', eulerangle[1]).as_matrix()\n",
    "#         # Z = R.from_euler('Z', eulerangle[0]).as_matrix()\n",
    "#         rotation_matrix = Z @ Y @ X\n",
    "#         # rotation_matrix = R.from_euler('xyz', eulerangle).as_matrix()\n",
    "#         # e_rotation_matrix = np.matmul(essential_matrix, rotation_matrix)\n",
    "#         # e_rotation_matrix = np.matmul(rotation_matrix, essential_matrix)\n",
    "#         translation = str2list(calib[6])\n",
    "#         fpd = str2list(calib[8])\n",
    "#         intrinsic = np.asarray([fpd[0], 0, fpd[2], 0,\n",
    "#                                 0, fpd[1], fpd[3], 0,\n",
    "#                                 0, 0, 1, 0]).reshape(3,4)\n",
    "#         extrinsic = np.hstack([rotation_matrix, np.asarray(translation).reshape(3, -1)])\n",
    "\n",
    "\n",
    "#         p2 = intrinsic.reshape(-1).tolist()\n",
    "#         R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "#         Tr_velo_to_cam = extrinsic.reshape(-1).tolist()\n",
    "#         Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "#         calib_kitti =  ['P0: '+list2str(p2), \n",
    "#                         'P1: '+list2str(p2), \n",
    "#                         'P2: '+list2str(p2), \n",
    "#                         'P3: '+list2str(p2), \n",
    "#                         'R0_rect: '+list2str(R0_rect), \n",
    "#                         'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "#                         'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "#         with open(f'/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/calib/{clip}_{frame}.txt', 'w') as f:\n",
    "#             f.write('\\n'.join(calib_kitti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calib_json = {\n",
    "#     \"extrinsic\": np.asarray(np.vstack([extrinsic, np.asarray([0, 0, 0, 1])]), dtype=np.float32).flatten().tolist(),\n",
    "#     \"intrinsic\": np.asarray(p2, dtype=np.float32).reshape(-1, 4)[:3, :3].flatten().tolist()\n",
    "# }\n",
    "# json_object = json.dumps(calib_json, indent = 4) \n",
    "# # Writing to sample.json \n",
    "# with open('/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_0000/calib/camera/front.json', \"w\") as outfile: \n",
    "#     outfile.write(json_object) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 2D_yolov5\n",
    "---\n",
    "- = 2D_rrc_Car_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_2d_label = f'{save_path}/2D_yolov5/'\n",
    "os.makedirs(save_2d_label, exist_ok=True)\n",
    "\n",
    "w = 1920\n",
    "h = 1200\n",
    "for scene in scenes:\n",
    "    labels = sorted(glob.glob(f'/NIA50/50-2/result/yolov5/test/labels/{scene}*.txt'))\n",
    "\n",
    "    label_df = pd.DataFrame()\n",
    "    for label in labels:\n",
    "        # with open(label, 'r') as f:\n",
    "        #     bbox = [re.sub('\\n', '', j) for j in f.readlines()]\n",
    "        frame_df = pd.read_csv(label, header=None, sep=' ')\n",
    "        frame_df.columns = ['frame', 'x', 'y', 'w', 'h', 'conf']\n",
    "        frame_df['frame'] = int(re.findall('[0-9]+', label)[-1])\n",
    "        frame_df['xmin'] = (frame_df['x'] - frame_df['w']/2) * w\n",
    "        frame_df['ymin'] = (frame_df['y'] - frame_df['h']/2) * h\n",
    "        frame_df['xmax'] = (frame_df['x'] + frame_df['w']/2) * w\n",
    "        frame_df['ymax'] = (frame_df['y'] + frame_df['h']/2) * h\n",
    "\n",
    "        label_df = pd.concat((label_df, frame_df[['frame', 'xmin', 'ymin', 'xmax', 'ymax', 'conf']]), axis=0)\n",
    "    label_df.to_csv(save_2d_label+f'{scene}.txt', index=None, header=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_2d_label = f'{save_path}/2D_yolov5/'\n",
    "# os.makedirs(save_2d_label, exist_ok=True)\n",
    "\n",
    "# w = 1920\n",
    "# h = 1200\n",
    "# for scene in scenes:\n",
    "#     labels = sorted(glob.glob(f'/data/NIA50/50-2/models/yolov5/runs/nia50_train1st/{scene}/labels/*.txt'))\n",
    "\n",
    "#     label_df = pd.DataFrame()\n",
    "#     for i, label in enumerate(labels):\n",
    "#         # with open(label, 'r') as f:\n",
    "#         #     bbox = [re.sub('\\n', '', j) for j in f.readlines()]\n",
    "#         frame_df = pd.read_csv(label, header=None, sep=' ')\n",
    "#         frame_df[0] = i\n",
    "#         frame_df.columns = ['frame', 'x', 'y', 'w', 'h', 'conf']\n",
    "#         frame_df['xmin'] = (frame_df['x'] - frame_df['w']/2) * w\n",
    "#         frame_df['ymin'] = (frame_df['y'] - frame_df['h']/2) * h\n",
    "#         frame_df['xmax'] = (frame_df['x'] + frame_df['w']/2) * w\n",
    "#         frame_df['ymax'] = (frame_df['y'] + frame_df['h']/2) * h\n",
    "\n",
    "#         label_df = pd.concat((label_df, frame_df[['frame', 'xmin', 'ymin', 'xmax', 'ymax', 'conf']]), axis=0)\n",
    "#     label_df.to_csv(save_2d_label+f'{scene}.txt', index=None, header=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def xywhn2xyxy(x, w=1920, h=1200):\n",
    "#     # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "#     x = np.array(x).reshape(1, -1)\n",
    "#     y = np.copy(x)\n",
    "#     y[:, 0] = (x[:, 0] - x[:, 2] / 2) * w  # top left x\n",
    "#     y[:, 1] = (x[:, 1] - x[:, 3] / 2) * h  # top left y\n",
    "#     y[:, 2] = (x[:, 0] + x[:, 2] / 2) * w  # bottom right x\n",
    "#     y[:, 3] = (x[:, 1] + x[:, 3] / 2) * h  # bottom right y\n",
    "#     y = list(np.around(y).astype(int).reshape(-1))\n",
    "#     return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # 3D_pvrcnn\n",
    "---\n",
    "- = 3D_pointrcnn_Car_Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 숫자리스트로 바꾸는 함수\n",
    "def str2list(txt):\n",
    "    txt = txt.replace('\\n', '').split(' ')[1:]\n",
    "    txt = list(map(float, txt))\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "# 리스트를 문자열로 바꾸는 함수\n",
    "def list2str(list):\n",
    "    list = ' '.join(map(str, list))\n",
    "    \n",
    "    return list\n",
    "\n",
    "\n",
    "# alpha 구하는 공식\n",
    "import math\n",
    "\n",
    "def normalizeAngle(angle):\n",
    "    result = angle % (2*math.pi)\n",
    "    if result < -math.pi:\n",
    "        result += 2*math.pi\n",
    "    elif result > math.pi:\n",
    "        result -= 2*math.pi\n",
    "    return result\n",
    "\n",
    "def cal_alpha_ori(x, z, ry):  \n",
    "    angle = ry\n",
    "    angle -= -math.atan2(z, x) -1.5*math.pi \n",
    "    alpha = normalizeAngle(angle)\n",
    "    return alpha # -1.818032754845337\n",
    "\n",
    "\n",
    "def roty(t, Rx=90/180*np.pi):\n",
    "    ''' Rotation about the y-axis. '''\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "    # return  np.array([[c, 0, s],\n",
    "    #                 [0, 1, 0],\n",
    "    #                 [-s, 0, c]])\n",
    "\n",
    "    X = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(Rx), -np.sin(Rx)],\n",
    "                    [0, np.sin(Rx), np.cos(Rx)]])\n",
    "\n",
    "    # X = np.eye(3)\n",
    "\n",
    "    Z = np.array([[c, -s, 0],\n",
    "                    [s, c, 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    return np.matmul(Z, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_3d_label = save_path + '/3D_pvrcnn/'\n",
    "os.makedirs(save_3d_label, exist_ok=True)\n",
    "\n",
    "with open('/data/NIA50/50-2/result/pvrcnn_allcat/test/eval/result.pkl', 'rb') as f:\n",
    "    results = pkl.load(f)\n",
    "\n",
    "for scene in scenes:\n",
    "\n",
    "    label_df = pd.DataFrame()\n",
    "    for result in results:\n",
    "        frame_df = pd.DataFrame(columns=['frame_id', 'type', 'x1', 'y1', 'x2', 'y2', 'score', 'h', 'w', 'l', 'x', 'y', 'z', 'rot_y'])\n",
    "        if scene in result['frame_id']:\n",
    "            frame_df['type'] = result['pred_labels']\n",
    "            frame_df['score'] = result['score']\n",
    "            frame_df['h'] = result['boxes_lidar'][:, 5]\n",
    "            frame_df['w'] = result['boxes_lidar'][:, 4]\n",
    "            frame_df['l'] = result['boxes_lidar'][:, 3]\n",
    "            frame_df['x'] = result['boxes_lidar'][:, 0]\n",
    "            frame_df['y'] = result['boxes_lidar'][:, 1]\n",
    "            frame_df['z'] = result['boxes_lidar'][:, 2]\n",
    "            frame_df['rot_y'] = result['boxes_lidar'][:, 6]\n",
    "            frame_df['frame_id'] = result['frame_id']\n",
    "\n",
    "        label_df = pd.concat([label_df, frame_df], axis=0)\n",
    "\n",
    "    label_df['scene'] = label_df['frame_id'].apply(lambda x: x[:-5])\n",
    "    label_df['frame'] = label_df['frame_id'].apply(lambda x: int(x[-4:]))\n",
    "    # label_df = label_df.loc[(-75.2 < label_df['x']) & (label_df['x'] < 75.2) & (-150.4 < label_df['y']) & (label_df['y'] < 0) & (-4 < label_df['z']) & (label_df['z'] < 8)]\n",
    "\n",
    "    with open(save_path+f'calib/{scene}.txt') as f:\n",
    "        calib = [re.sub('\\n', '', i) for i in f.readlines()]\n",
    "    intrinsic = np.asarray(calib[2].split(' ')[1:], dtype=float).reshape(3, 4)\n",
    "    extrinsic = np.asarray(calib[5].split(' ')[1:], dtype=float).reshape(3, 4)\n",
    "    extrinsic = np.vstack([extrinsic, [0, 0, 0, 1]])\n",
    "    # scene_df = label_df.loc[label_df['scene']==scene]\n",
    "\n",
    "    minx = []\n",
    "    miny = []\n",
    "    maxx = []\n",
    "    maxy = []\n",
    "    for i in np.arange(len(label_df)):\n",
    "        obj = label_df.iloc[i]\n",
    "\n",
    "        R = roty(obj['rot_y'])\n",
    "        x = obj['x']\n",
    "        y = obj['y']\n",
    "        z = obj['z']\n",
    "        l = obj['l']\n",
    "        w = obj['w']\n",
    "        h = obj['h']\n",
    "        \n",
    "        x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2];\n",
    "        y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2];\n",
    "        z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2];\n",
    "        \n",
    "        corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "        corners_3d[0, :] = corners_3d[0, :] + x  # x\n",
    "        corners_3d[1, :] = corners_3d[1, :] + y  # y\n",
    "        corners_3d[2, :] = corners_3d[2, :] + z  # z\n",
    "        corners_3d = np.vstack([corners_3d, [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "        \n",
    "        point2d = np.matmul(intrinsic, np.matmul(extrinsic, corners_3d))\n",
    "        pointx = np.around(point2d/point2d[2])[0]\n",
    "        pointy = np.around(point2d/point2d[2])[1]\n",
    "        pointx[pointx > 1920] = 1920\n",
    "        pointx[pointx < 0] = 0\n",
    "        pointy[pointy > 1200] = 1200\n",
    "        pointy[pointy < 0] = 0\n",
    "\n",
    "        minx.append(min(pointx))\n",
    "        miny.append(min(pointy))\n",
    "        maxx.append(max(pointx))\n",
    "        maxy.append(max(pointy))\n",
    "\n",
    "    label_df[['x1', 'y1', 'x2', 'y2']] = np.asarray((minx, miny, maxx, maxy)).T\n",
    "    label_df['alpha'] = 0\n",
    "    # label_df = label_df.loc[(label_df['x2']-label_df['x1']!=0) & (label_df['y2']-label_df['y1']!=0)]\n",
    "    label_df = label_df[['frame', 'type', 'x1', 'y1', 'x2', 'y2', 'score', 'h', 'w', 'l', 'x', 'y', 'z', 'rot_y', 'alpha']]\n",
    "    label_df.to_csv(save_3d_label+f'{scene}.txt', index=None, header=None, sep=',')\n",
    "\n",
    "    # scene_df[['x1', 'y1', 'x2', 'y2']] = np.asarray((minx, miny, maxx, maxy)).T\n",
    "    # scene_df['alpha'] = 0\n",
    "    # scene_df = scene_df[['frame', 'type', 'x1', 'y1', 'x2', 'y2', 'score', 'h', 'w', 'l', 'x', 'y', 'z', 'rot_y', 'alpha']]\n",
    "    # scene_df.to_csv(save_3d_label+f'{scene}.txt', index=None, header=None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_3d_label = save_path + '/3D_pvrcnn/'\n",
    "# os.makedirs(save_3d_label, exist_ok=True)\n",
    "\n",
    "# with open('/data/NIA50/50-2/models/OpenPCD/output/nia50_train1st_pvrcnn/result.pkl', 'rb') as f:\n",
    "#     results = pkl.load(f)\n",
    "\n",
    "# dev = (len(results)//1000) + 1\n",
    "# for idx in np.arange(dev):\n",
    "#     start = int(idx*1000)\n",
    "#     end = int((idx+1)*1000)\n",
    "\n",
    "#     label_df = pd.DataFrame()\n",
    "#     for result in results[start:end]:\n",
    "#         frame_df = pd.DataFrame(columns=['frame_id', 'type', 'x1', 'y1', 'x2', 'y2', 'score', 'h', 'w', 'l', 'x', 'y', 'z', 'rot_y'])\n",
    "#         frame_df['type'] = result['pred_labels']\n",
    "#         frame_df['score'] = result['score']\n",
    "#         frame_df['h'] = result['boxes_lidar'][:, 5]\n",
    "#         frame_df['w'] = result['boxes_lidar'][:, 4]\n",
    "#         frame_df['l'] = result['boxes_lidar'][:, 3]\n",
    "#         frame_df['x'] = result['boxes_lidar'][:, 1]\n",
    "#         frame_df['y'] = result['boxes_lidar'][:, 0] * -1\n",
    "#         frame_df['z'] = result['boxes_lidar'][:, 2]\n",
    "#         frame_df['rot_y'] = result['boxes_lidar'][:, 6]\n",
    "#         frame_df['frame_id'] = result['frame_id']\n",
    "\n",
    "#         label_df = pd.concat([label_df, frame_df], axis=0)\n",
    "\n",
    "#     label_df['scene'] = label_df['frame_id'].apply(lambda x: x[:-5])\n",
    "#     label_df['frame'] = label_df['frame_id'].apply(lambda x: int(x[-4:]))\n",
    "\n",
    "#     for scene in label_df['scene'].unique():\n",
    "#         with open(save_path+f'calib/{scene}.txt') as f:\n",
    "#             calib = [re.sub('\\n', '', i) for i in f.readlines()]\n",
    "#         intrinsic = np.asarray(calib[2].split(' ')[1:], dtype=float).reshape(3, 4)\n",
    "#         extrinsic = np.asarray(calib[5].split(' ')[1:], dtype=float).reshape(4, 4)\n",
    "#         scene_df = label_df.loc[label_df['scene']==scene]\n",
    "\n",
    "#         minx = []\n",
    "#         miny = []\n",
    "#         maxx = []\n",
    "#         maxy = []\n",
    "#         for i in np.arange(len(scene_df)):\n",
    "#             obj = scene_df.iloc[i]\n",
    "\n",
    "#             R = roty(obj['rot_y'])\n",
    "#             x = obj['x']\n",
    "#             y = obj['y']\n",
    "#             z = obj['z']\n",
    "#             l = obj['l']\n",
    "#             w = obj['w']\n",
    "#             h = obj['h']\n",
    "            \n",
    "#             x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2];\n",
    "#             y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2];\n",
    "#             z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2];\n",
    "            \n",
    "#             corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "#             corners_3d[0, :] = corners_3d[0, :] + x  # x\n",
    "#             corners_3d[1, :] = corners_3d[1, :] + y  # y\n",
    "#             corners_3d[2, :] = corners_3d[2, :] + z  # z\n",
    "#             corners_3d = np.vstack([corners_3d, [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "            \n",
    "#             point2d = np.matmul(intrinsic, np.matmul(extrinsic, corners_3d))\n",
    "#             pointx = np.around(point2d/point2d[2])[0]\n",
    "#             pointy = np.around(point2d/point2d[2])[1]\n",
    "#             pointx[pointx > 1920] = 1920\n",
    "#             pointx[pointx < 0] = 0\n",
    "#             pointy[pointy > 1200] = 1200\n",
    "#             pointy[pointy < 0] = 0\n",
    "\n",
    "#             minx.append(min(pointx))\n",
    "#             miny.append(min(pointy))\n",
    "#             maxx.append(max(pointx))\n",
    "#             maxy.append(max(pointy))\n",
    "\n",
    "#         scene_df[['x1', 'y1', 'x2', 'y2']] = np.asarray((minx, miny, maxx, maxy)).T\n",
    "#         scene_df['alpha'] = 0\n",
    "#         scene_df = scene_df[['frame', 'type', 'x1', 'y1', 'x2', 'y2', 'score', 'h', 'w', 'l', 'x', 'y', 'z', 'rot_y', 'alpha']]\n",
    "#         scene_df.to_csv(save_3d_label+f'{scene}.txt', index=None, header=None, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # image_02\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image = save_path + '/image_02/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "for scene in scenes:\n",
    "    img_src = source_path+scene+'/camera/camera_0'\n",
    "    shutil.copytree(img_src, save_image+scene, dirs_exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # velodyne\n",
    "---\n",
    "- 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_velo = save_path + 'velodyne'\n",
    "# os.makedirs(save_velo, exist_ok=True)\n",
    "\n",
    "# for scene in scenes:\n",
    "#     os.makedirs(save_velo+'/'+scene, exist_ok=True)\n",
    "#     velodynes = sorted(glob.glob(source_path+scene+'/lidar/*.pcd'))\n",
    "#     for velodyne in velodynes:\n",
    "#         # clip = re.search('Suwon_[a-zA-Z0-9_]+', velodyne).group()\n",
    "#         # clip = re.findall('[a-zA-Z0-9_]+', velodyne)[-4]\n",
    "#         frame = re.search('[0-9]+.pcd', velodyne).group().replace('.pcd', '')\n",
    "        \n",
    "#         pcd = o3d.t.io.read_point_cloud(velodyne)\n",
    "#         positions = pcd.point.positions.numpy()\n",
    "#         intensity = pcd.point.intensity.numpy()\n",
    "#         pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "#         pcd_bytes = pcd.tobytes()\n",
    "        \n",
    "#         with open(save_velo+f'/{scene}/{frame}.bin', 'wb') as f:\n",
    "#             f.write(pcd_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/velodyne', exist_ok=True)\n",
    "\n",
    "# path = '/data/NIA50/50-2/data/NIA50_samples/'\n",
    "# clips = sorted(os.listdir(path))\n",
    "# for clip in clips[1:2]:\n",
    "#     # frames = sorted(os.listdir(f'/data/NIA50/50-2/data/NIA50_samples/{clip}/lidar'))    \n",
    "#     pcd_files = sorted(glob.glob(path+f'{clip}/lidar/*.pcd'))\n",
    "#     for pcd_file in pcd_files:\n",
    "#         frame = pcd_file[-8:-4]\n",
    "#         pcd = o3d.t.io.read_point_cloud(pcd_file)\n",
    "#         positions = pcd.point.positions.numpy()\n",
    "#         intensity = pcd.point.intensity.numpy()\n",
    "        \n",
    "#         pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "#         pcd_bytes = pcd.tobytes()\n",
    "        \n",
    "#         with open(f'/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/velodyne/{clip}_{frame}.bin', 'wb') as f:\n",
    "#             f.write(pcd_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/velodyne/{clip}_{frame}.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # PointRCNN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '/data/NIA50/50-2/data/NIA50/train_1st/raw/*/'\n",
    "source_save_path = '/data/NIA50/50-2/data/NIA50/train_1st/pointrcnn/training/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # velodyne\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{source_save_path}/velodyne/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "velodynes = sorted(glob.glob(f'{source_path}lidar/*.pcd'))\n",
    "\n",
    "for velodyne in velodynes:\n",
    "    # clip = re.search('Suwon_[a-zA-Z0-9_]+', velodyne).group()\n",
    "    clip = re.findall('[a-zA-Z0-9_]+', velodyne)[-4]\n",
    "    frame = re.search('[0-9]+.pcd', velodyne).group().replace('.pcd', '')\n",
    "    \n",
    "    pcd = o3d.t.io.read_point_cloud(velodyne)\n",
    "    positions = pcd.point.positions.numpy()\n",
    "    intensity = pcd.point.intensity.numpy()\n",
    "    pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "    pcd_bytes = pcd.tobytes()\n",
    "    \n",
    "    # os.makedirs(save_path+clip, exist_ok=True)\n",
    "    with open(save_path+f'{clip}_{frame}.bin', 'wb') as f:\n",
    "        f.write(pcd_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # calib\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{source_save_path}/calib/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "calibs = sorted(glob.glob(f'{source_path}/calib/camera/*.json'))\n",
    "frames = [str(i).zfill(4) for i in np.arange(10).tolist()]\n",
    "\n",
    "for calib in calibs:\n",
    "    # clip = re.search('Suwon_[a-zA-Z0-9_]+', calib).group()\n",
    "    scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "    # img_path = '/data/NIA50/SUSTechPOINTS_2-050/data/Suwon_A_2210261635_0001_calib_ok/camera/camera_0'\n",
    "    # frame = re.search('[0-9]+.pcd', calib).group().replace('.json', '')\n",
    "\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "\n",
    "    p2 = np.hstack([np.asarray(calib['intrinsic']).reshape(3, 3), np.asarray([0, 0, 0]).reshape(3, -1)]).flatten().tolist()\n",
    "    R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "    # viewmatrix = np.asarray([0, -1, 0,\n",
    "    #                          0, 0, -1,\n",
    "    #                          -1, 0, 0]).reshape(3, 3)\n",
    "    # extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "    # proj_matrix = np.matmul(viewmatrix, extrinsic[:3, :3])\n",
    "    # extrinsic[:3, :3] = proj_matrix\n",
    "    # Tr_velo_to_cam = extrinsic.reshape(-1).tolist()[:12]\n",
    "    # Tr_velo_to_cam = proj_matrix.reshape(-1).tolist()[:12]\n",
    "    Tr_velo_to_cam = calib['extrinsic'][:12]\n",
    "    Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "    calib_kitti =  ['P0: '+list2str(p2), \n",
    "                    'P1: '+list2str(p2), \n",
    "                    'P2: '+list2str(p2), \n",
    "                    'P3: '+list2str(p2), \n",
    "                    'R0_rect: '+list2str(R0_rect), \n",
    "                    'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "                    'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "    for frame in frames:\n",
    "        with open(save_path+f'{scene}_{frame}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(calib_kitti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # label_2\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roty(t, Rx=90/180*np.pi):\n",
    "    ''' Rotation about the y-axis. '''\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "    # return  np.array([[c, 0, s],\n",
    "    #                 [0, 1, 0],\n",
    "    #                 [-s, 0, c]])\n",
    "\n",
    "    X = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(Rx), -np.sin(Rx)],\n",
    "                    [0, np.sin(Rx), np.cos(Rx)]])\n",
    "\n",
    "    Z = np.array([[c, -s, 0],\n",
    "                    [s, c, 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    return np.matmul(Z, X)\n",
    "\n",
    "\n",
    "# 문자열 숫자리스트로 바꾸는 함수\n",
    "def str2list(txt):\n",
    "    txt = txt.replace('\\n', '').split(' ')[1:]\n",
    "    txt = list(map(float, txt))\n",
    "    \n",
    "    return txt\n",
    "\n",
    "\n",
    "# 리스트를 문자열로 바꾸는 함수\n",
    "def list2str(list):\n",
    "    list = ' '.join(map(str, list))\n",
    "    \n",
    "    return list\n",
    "\n",
    "\n",
    "# alpha 구하는 공식\n",
    "import math\n",
    "\n",
    "def normalizeAngle(angle):\n",
    "    result = angle % (2*math.pi)\n",
    "    if result < -math.pi:\n",
    "        result += 2*math.pi\n",
    "    elif result > math.pi:\n",
    "        result -= 2*math.pi\n",
    "    return result\n",
    "\n",
    "def cal_alpha_ori(x, z, ry):  \n",
    "    angle = ry\n",
    "    angle -= -math.atan2(z, x) -1.5*math.pi \n",
    "    alpha = normalizeAngle(angle)\n",
    "    return alpha # -1.818032754845337\n",
    "\n",
    "\n",
    "def xyxy2xywhn(x, w=1920, h=1200, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    if clip:\n",
    "        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    y = list(y.reshape(-1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'{source_save_path}/label_2/'\n",
    "calib_path = f'{source_save_path}/calib/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "type_list = []\n",
    "rename = {'Adult': 'Pedestrian', \n",
    "          'Bus': 'Car',\n",
    "          'Car': 'Car',\n",
    "          'Kid': 'Pedestrian',\n",
    "          'Kickboard': 'Cyclist',\n",
    "          'Large_Truck': 'Car',\n",
    "          'Light_Car': 'Car',\n",
    "          'Medium_Truck': 'Car',\n",
    "          'Mini_Bus': 'Car',\n",
    "        #   'Motorcycle': 'MOTORCYCLE',\n",
    "        #   'Pedestrian': 'Pedestrian',\n",
    "          'SUV': 'Car',\n",
    "          'Small_Car': 'Car',\n",
    "          'Small_Truck': 'Car',\n",
    "          'Special_Vehicle': 'Car',\n",
    "          'Two_Wheeler': 'Cyclist',\n",
    "          'Van': 'Car'}\n",
    "\n",
    "labels = sorted(glob.glob(f'{source_path}/label/[0-9]*.json'))\n",
    "\n",
    "for label in labels:\n",
    "    try:\n",
    "        # clip = re.search('Suwon_[a-zA-Z0-9_]+', label).group()\n",
    "        clip = re.findall('[a-zA-Z0-9_]+', label)[-4]\n",
    "\n",
    "        frame = re.search('[0-9]+.json', label).group().replace('.json', '')\n",
    "\n",
    "        with open(calib_path+f'{clip}_{frame}.txt', 'r') as f:\n",
    "            calib = f.readlines()\n",
    "            # calib = [re.sub('\\n', '', i) for i in calib]\n",
    "\n",
    "        # extrinsic = np.asarray(str2list(calib[5])).reshape(4, 4)[:3, :4]\n",
    "        intrinsic = np.asarray(str2list(calib[2])).reshape(3, 4)[:3, :3]\n",
    "        extrinsic = np.asarray(str2list(calib[5])).reshape(3, 4)\n",
    "        \n",
    "        with open(label, 'r') as f:\n",
    "            label = json.load(f)\n",
    "        \n",
    "        label_txt = []\n",
    "        for i in np.arange(len(label)):\n",
    "            try:\n",
    "                # frame_num = [int(frame)]\n",
    "                # id_ = [label[i]['obj_id']]\n",
    "                type_ = label[i]['obj_type']\n",
    "                rename_type = [rename[type_]]\n",
    "                truncation = [0.0]\n",
    "                occuluded = [0]\n",
    "                alpha = 0\n",
    "                # bbox = [0, 0, 0, 0]\n",
    "                lwh = list(label[i]['psr']['scale'].values())\n",
    "                dimensions = [lwh[2], lwh[1], lwh[0]]\n",
    "                xyz = list(label[i]['psr']['position'].values())\n",
    "                location = np.matmul(extrinsic, np.asarray((xyz + [0])).reshape(4, -1)).flatten().tolist()\n",
    "                rotation_y = [label[i]['psr']['rotation']['z']]\n",
    "\n",
    "                # make 2dbox labeling\n",
    "                l = lwh[0]\n",
    "                w = lwh[1]\n",
    "                h = lwh[2]\n",
    "\n",
    "                x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2];\n",
    "                y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2];\n",
    "                z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2];\n",
    "\n",
    "                R = roty(rotation_y[0])\n",
    "                corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "                corners_3d[0, :] = corners_3d[0, :] + xyz[0]  # x\n",
    "                corners_3d[1, :] = corners_3d[1, :] + xyz[1]  # y\n",
    "                corners_3d[2, :] = corners_3d[2, :] + xyz[2]  # z\n",
    "                corners_3d = np.vstack([corners_3d, [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "\n",
    "                point2d = np.matmul(intrinsic, np.matmul(extrinsic, corners_3d))\n",
    "                pointx = np.around(point2d/point2d[2])[0]\n",
    "                pointx[pointx > 1920] = 1920\n",
    "                pointx[pointx < 0] = 0\n",
    "                pointy = np.around(point2d/point2d[2])[1]\n",
    "                pointy[pointy > 1200] = 1200\n",
    "                pointy[pointy < 0] = 0\n",
    "                bbox = [min(pointx), min(pointy), max(pointx), max(pointy)]\n",
    "                # bbox = xyxy2xywhn(box)\n",
    "\n",
    "                alpha = [cal_alpha_ori(location[0], location[2], rotation_y[0])]\n",
    "                \n",
    "                # label_format = list2str(rename_type + truncation + occuluded + alpha + bbox + dimensions + xyz + rotation_y)\n",
    "                label_format = list2str(rename_type + truncation + occuluded + alpha + bbox + dimensions + location + rotation_y)\n",
    "                label_txt.append(label_format)\n",
    "                \n",
    "                type_list.append(rename_type[0])\n",
    "\n",
    "            except KeyError:\n",
    "                print('라벨링 에러 :', clip, frame, type_)\n",
    "            \n",
    "            except TypeError:\n",
    "                print('타입 에러 : ', clip, frame, type_)\n",
    "\n",
    "        # os.makedirs(save_path+clip, exist_ok=True)\n",
    "        with open(save_path+f'/{clip}_{frame}.txt', 'w') as f:\n",
    "            f.write('\\n'.join(label_txt))\n",
    "        \n",
    "    except ValueError:\n",
    "        print('json 에러 :', clip, frame)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print('calib 에러 :', clip, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # image_02\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msource_save_path\u001b[39m}\u001b[39;00m\u001b[39m/image_2/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m os\u001b[39m.\u001b[39mmakedirs(save_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m img_ls \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(glob\u001b[39m.\u001b[39mglob(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msource_path\u001b[39m}\u001b[39;00m\u001b[39m/camera/camera_0/*.jpg\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_save_path' is not defined"
     ]
    }
   ],
   "source": [
    "save_path = f'{source_save_path}/image_2/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "img_ls = sorted(glob.glob(f'{source_path}/camera/camera_0/*.jpg'))\n",
    "\n",
    "for img in img_ls:\n",
    "    clip = re.findall('[a-zA-Z0-9_]+', img)[-5]\n",
    "    # clip = re.search('Suwon_[a-zA-Z0-9_]+', img).group()\n",
    "    \n",
    "    frame = re.search('[0-9]+.jpg', img).group().replace('.jpg', '')\n",
    "\n",
    "    shutil.copy(img, save_path+f'{clip}_{frame}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # ImageSets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈 라벨 걸러내기\n",
    "import os\n",
    "\n",
    "labels = sorted(glob.glob('/data/NIA50/50-2/data/NIA50/train_1st/pointrcnn/training/label_2/*.txt'))\n",
    "\n",
    "empty_labels = []\n",
    "for label in labels:\n",
    "    with open(label, 'r') as f:\n",
    "        labeled = f.readlines()\n",
    "    \n",
    "    if len(labeled) == 0:\n",
    "        # print(label)\n",
    "        empty_labels.append(re.findall('[a-zA-Z0-9_]+', label)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/NIA50/50-2/data/NIA50/train_1st/yolov5/ImageSets/train.txt', 'r') as f:\n",
    "    train = f.readlines()\n",
    "with open('/data/NIA50/50-2/data/NIA50/train_1st/yolov5/ImageSets/val.txt', 'r') as f:\n",
    "    val = f.readlines()\n",
    "\n",
    "\n",
    "train = sorted([re.findall('[a-zA-Z0-9_]+', i)[-2] for i in train])\n",
    "train = [i for i in train if i not in empty_labels]\n",
    "val = sorted([re.findall('[a-zA-Z0-9_]+', i)[-2] for i in val])\n",
    "val = [i for i in val if i not in empty_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/data/NIA50/50-2/data/NIA50/train_1st/pointrcnn/ImageSets/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# clips = sorted([i.replace('.txt', '') for i in os.listdir('/data/NIA50/50-2/data/mot_nia50/pointrcnn/training/calib')])\n",
    "\n",
    "with open(save_path+'train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(train))\n",
    "\n",
    "with open(save_path+'val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # kitti_infos_test.pkl\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/NIA50/50-2/data/mot_nia50/pointrcnn/testing/'\n",
    "clips = sorted(os.listdir(path+'image_2'))\n",
    "\n",
    "test_infos = []\n",
    "for clip in clips:\n",
    "    img_ls = sorted(glob.glob(path+f'image_2/{clip}/*.jpg'))\n",
    "\n",
    "    with open(path+f'/calib/{clip}.txt') as f:\n",
    "        calib = f.readlines()\n",
    "\n",
    "    P2 = np.asarray(str2list(calib[2])).reshape(3, 4)\n",
    "    P2 = np.vstack([P2, np.asarray([0, 0, 0, 1])])\n",
    "    R0_rect = np.eye(4)\n",
    "    Tr_velo_to_cam = np.asarray(str2list(calib[5])).reshape(4, 4)\n",
    "\n",
    "    for img in img_ls:\n",
    "        idx = re.search('Suwon[a-zA-Z0-9_/]+', img).group()\n",
    "        img_shape = np.asarray(cv2.imread(img).shape[:2], dtype=np.int32)\n",
    "\n",
    "        info = {'point_cloud': {'num_features': 4, 'lidar_idx': idx},\n",
    "         'image': {'image_idx': idx, 'image_shape': img_shape},\n",
    "         'calib': {'P2': P2,\n",
    "         'R0_rect': R0_rect,\n",
    "         'Tr_velo_to_cam': Tr_velo_to_cam}}\n",
    "\n",
    "        test_infos.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/NIA50/50-2/data/mot_nia50/pointrcnn/kitti_infos_test.pkl', 'wb') as f:\n",
    "    pkl.dump(test_infos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(cv2.imread(img).shape[:2], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # PV-RCNN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "\n",
    "src_path = '/data/NIA50/50-2/data/NIA50/nia50_all/raw'\n",
    "calibs = sorted(glob.glob(f'{src_path}/*/calib/camera/camera_0.json'))\n",
    "\n",
    "calib_ls = []\n",
    "scenes = []\n",
    "for calib in calibs:\n",
    "    scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    if calib['extrinsic'] not in calib_ls:\n",
    "        calib_ls.append(calib['extrinsic'])\n",
    "        scenes.append(scene)\n",
    "    \n",
    "calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "             'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "             'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "             'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # points\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/data/NIA50/50-2/data/NIA50/nia50_all/raw/'\n",
    "save_path = '/data/NIA50/50-2/data/NIA50/nia50_all/pvrcnn_integ/points/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "points = sorted(glob.glob(f'{src_path}/*/lidar/*.pcd'))\n",
    "\n",
    "for point in points:\n",
    "    scene = re.findall('[a-zA-Z_0-9_]+', point)[-4]\n",
    "    frame = re.findall('[0-9]+', point)[-1]\n",
    "    \n",
    "    with open(f'{src_path}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    \n",
    "    pcd = o3d.t.io.read_point_cloud(point)\n",
    "    positions = pcd.point.positions.numpy()\n",
    "    intensity = pcd.point.intensity.numpy()\n",
    "\n",
    "    if calib['extrinsic'] == calib_typ['typ1']['calib']: positions[:, 2] += calib_typ['typ1']['mov_zpoint']\n",
    "    elif calib['extrinsic'] == calib_typ['typ2']['calib']: positions[:, 2] += calib_typ['typ2']['mov_zpoint']\n",
    "    elif calib['extrinsic'] == calib_typ['typ3']['calib']: positions[:, 2] += calib_typ['typ3']['mov_zpoint']\n",
    "    elif calib['extrinsic'] == calib_typ['typ4']['calib']: positions[:, 2] += calib_typ['typ4']['mov_zpoint']\n",
    "\n",
    "    pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "    np.save(save_path+f'/{scene}_{frame}.npy', pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points = sorted(glob.glob('/data/NIA50/50-2/data/NIA50_real/*/lidar/*.pcd'))\n",
    "# save_path = '/data/NIA50/50-2/data/mot_nia50/pvrcnn/points/'\n",
    "\n",
    "# for point in points:\n",
    "#     clip = re.search('Suwon_[A-Z_0-9]+', point).group()\n",
    "#     frame = re.search('[0-9]+.pcd', point).group().replace('.pcd', '')\n",
    "    \n",
    "#     pcd = o3d.t.io.read_point_cloud(point)\n",
    "#     positions = pcd.point.positions.numpy()\n",
    "#     intensity = pcd.point.intensity.numpy()\n",
    "#     pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "\n",
    "#     # npy = np.save(save_path+f'/{clip}_{frame}.npy', pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # labels\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x축 범위: -134.3012173784256 ~ 81.59058380126956\n",
      "y축 범위: -98.37752972578596 ~ 69.27335041106909\n",
      "z축 범위: -3.3815337419509883 ~ 10.622396979540696\n"
     ]
    }
   ],
   "source": [
    "src_path = '/data/NIA50/50-2/data/NIA50/nia50_all/raw/'\n",
    "save_path = '/data/NIA50/50-2/data/NIA50/nia50_all/pvrcnn_integ/labels/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "change_class_ls = {'Small_Car': 'Car',\n",
    "                    'Light_Car': 'Car',\n",
    "                    'Car': 'Car',\n",
    "                    'Van': 'Van',\n",
    "                    'SUV': 'SUV',\n",
    "                    'Small_Truck': 'Truck',\n",
    "                    'Medium_Truck': 'Truck',\n",
    "                    'Large_Truck': 'Truck',\n",
    "                    'Mini_Bus': 'Bus',\n",
    "                    'Bus': 'Bus',\n",
    "                    'Special_Vehicle': 'Special_Vehicle',\n",
    "                    'Two_Wheeler': 'Two_Wheeler',\n",
    "                    'Kickboard': 'Two_Wheeler',\n",
    "                    'Adult': 'Person',\n",
    "                    'Kid': 'Person'}\n",
    "\n",
    "labels = sorted(glob.glob(src_path+'*/label/*.json'))\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "for label in labels:\n",
    "    scene = re.findall('[a-zA-Z_0-9_]+', label)[-4]\n",
    "    frame = re.findall('[0-9]+', label)[-1]\n",
    "\n",
    "    with open(f'{src_path}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    \n",
    "    with open(label, 'r') as f:\n",
    "        label = json.load(f)\n",
    "    \n",
    "    labeling_ls = []\n",
    "    for i in np.arange(len(label)):\n",
    "        try:\n",
    "            xyz = list(label[i]['psr']['position'].values())\n",
    "\n",
    "            if calib['extrinsic'] == calib_typ['typ1']['calib']: xyz[2] += calib_typ['typ1']['mov_zpoint']\n",
    "            elif calib['extrinsic'] == calib_typ['typ2']['calib']: xyz[2] += calib_typ['typ2']['mov_zpoint']\n",
    "            elif calib['extrinsic'] == calib_typ['typ3']['calib']: xyz[2] += calib_typ['typ3']['mov_zpoint']\n",
    "            elif calib['extrinsic'] == calib_typ['typ4']['calib']: xyz[2] += calib_typ['typ4']['mov_zpoint']\n",
    "\n",
    "            lwh = list(label[i]['psr']['scale'].values())\n",
    "            rotation_y = [label[i]['psr']['rotation']['z']]\n",
    "            class_ = label[i]['obj_type']\n",
    "            # rename_class = [class_ls[class_]]\n",
    "            # label_format = ' '.join(map(str, xyz + lwh + rotation_y + rename_class))\n",
    "            label_format = ' '.join(map(str, xyz + lwh + rotation_y + [class_]))\n",
    "            labeling_ls.append(label_format)\n",
    "\n",
    "            # if xyz[0] < -130:\n",
    "            #     print('x축', scene, frame)\n",
    "\n",
    "            # if xyz[1] < -90:\n",
    "            #     print('y축', scene, frame)\n",
    "\n",
    "            # if xyz[2] > 8:\n",
    "            #     print('z축', scene, frame)\n",
    "\n",
    "            x.append(xyz[0])\n",
    "            y.append(xyz[1])\n",
    "            z.append(xyz[2])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    with open(save_path+f'/{scene}_{frame}.txt', 'w') as f:\n",
    "        f.write('\\n'.join(labeling_ls))\n",
    "\n",
    "#  객체 포인트 범위 확인\n",
    "print(f'x축 범위: {min(x)} ~ {max(x)}')\n",
    "print(f'y축 범위: {min(y)} ~ {max(y)}')\n",
    "print(f'z축 범위: {min(z)} ~ {max(z)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('/NIA50/50-2/data/nia50_all/pvrcnn/labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = sorted(glob.glob('/data/NIA50/50-2/data/NIA50_real/*/label/*.json'))\n",
    "# save_path = '/data/NIA50/50-2/data/mot_nia50/pvrcnn/labels/'\n",
    "\n",
    "# class_list = []\n",
    "# labels = sorted(glob.glob('/data/NIA50/50-2/data/NIA50_real/*/label/*.json'))\n",
    "# # save_path = '/data/NIA50/50-2/data/mot_nia50/pvrcnn/labels/'\n",
    "\n",
    "# df = pd.DataFrame(columns = ['clip', 'CAR', 'PEDESTRIAN', 'MOTORCYCLE'])\n",
    "# rename = {'Adult': 'PEDESTRIAN', \n",
    "#           'Bus': 'CAR',\n",
    "#           'Car': 'CAR',\n",
    "#           'Kid': 'PEDESTRIAN',\n",
    "#           'Kickboard': 'MOTORCYCLE',\n",
    "#           'Large_Truck': 'CAR',\n",
    "#           'Light_Car': 'CAR',\n",
    "#           'Medium_Truck': 'CAR',\n",
    "#           'Mini_Bus': 'CAR',\n",
    "#           'Motorcycle': 'MOTORCYCLE',\n",
    "#           'Pedestrian': 'PEDESTRIAN',\n",
    "#           'SUV': 'CAR',\n",
    "#           'Small_Car': 'CAR',\n",
    "#           'Small_Truck': 'CAR',\n",
    "#           'Special_Vehicle': 'CAR',\n",
    "#           'Two_Wheeler': 'MOTORCYCLE',\n",
    "#           'Van': 'CAR'}\n",
    "\n",
    "# error_clips = []\n",
    "# for idx, label in enumerate(labels):\n",
    "#     try:\n",
    "#         clip = re.search('Suwon_[A-Z_0-9]+', label).group()\n",
    "#         frame = re.search('[0-9]+.json', label).group().replace('.json', '')\n",
    "        \n",
    "#         with open(label, 'r') as f:\n",
    "#             label = json.load(f)\n",
    "        \n",
    "#         class_list = []\n",
    "#         # label_txt = []\n",
    "#         for i in np.arange(len(label)):\n",
    "#             try:\n",
    "#                 class_ = label[i]['obj_type']\n",
    "#                 rename_class = [rename[class_]]\n",
    "#                 class_list.append(rename_class[0])\n",
    "#             except KeyError:\n",
    "#                 print('라벨링 에러 :', clip, frame, class_)\n",
    "        \n",
    "#         cnt = Counter(class_list)\n",
    "#         df.loc[idx] = [clip, cnt['CAR'], cnt['PEDESTRIAN'], cnt['MOTORCYCLE']]\n",
    "#         # with open(save_path+f'/{clip}_{frame}.txt', 'w') as f:\n",
    "#         #     f.write('\\n'.join(label_txt))\n",
    "        \n",
    "#     except ValueError:\n",
    "#         print('json 에러 :', clip, frame)\n",
    "#         error_clips.append(clip)\n",
    "        \n",
    "# df_ = df.groupby(['clip']).sum().reset_index()\n",
    "\n",
    "# error_clip_idx = []\n",
    "# for error_clip in list(set(error_clips)):\n",
    "#     idx = df_.loc[df_['clip'] == error_clip].index.values\n",
    "#     error_clip_idx.append(int(idx))\n",
    "\n",
    "# df_ = df_.drop(error_clip_idx, axis=0)\n",
    "\n",
    "# for label in labels:\n",
    "#     try:\n",
    "#         clip = re.search('Suwon_[A-Z_0-9]+', label).group()\n",
    "#         frame = re.search('[0-9]+.json', label).group().replace('.json', '')\n",
    "        \n",
    "#         with open(label, 'r') as f:\n",
    "#             label = json.load(f)\n",
    "        \n",
    "#         label_txt = []\n",
    "#         for i in np.arange(len(label)):\n",
    "#             try:\n",
    "#             # print(clip)\n",
    "#             # print(frame)\n",
    "#                 xyz = list(label[i]['psr']['position'].values())\n",
    "#                 lwh = list(label[i]['psr']['scale'].values())\n",
    "#                 rotation_y = [label[i]['psr']['rotation']['z']]\n",
    "#                 class_ = label[i]['obj_type']\n",
    "#                 rename_class = [rename[class_]]\n",
    "#                 # if class_ == 'Special_Vehicle':\n",
    "#                 #     print(clip, frame)\n",
    "#                 # class_list.append(class_)\n",
    "#                 class_list.append(rename_class[0])\n",
    "#                 label_format = ' '.join(map(str, xyz + lwh + rotation_y + rename_class))\n",
    "#                 label_txt.append(label_format)\n",
    "#             except KeyError:\n",
    "#                 print('라벨링 에러 :', clip, frame, class_)\n",
    "\n",
    "#         with open(save_path+f'/{clip}_{frame}.txt', 'w') as f:\n",
    "#             f.write('\\n'.join(label_txt))\n",
    "        \n",
    "#     except ValueError:\n",
    "#         print('json 에러 :', clip, frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # ImageSets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/data/NIA50/50-2/data/NIA50/nia50_all/raw/'\n",
    "point_src_path = '/data/NIA50/50-2/data/NIA50/nia50_all/pvrcnn_integ/points'\n",
    "save_path = '/data/NIA50/50-2/data/NIA50/nia50_all/pvrcnn_integ/ImageSets/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# dat_typs = set([re.search('[A-Z]_[A-Z]', j).group() for j in os.listdir(src_path)])\n",
    "scenes = sorted(os.listdir(src_path))\n",
    "dat_typs = set([re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0] for scene in scenes])\n",
    "points = sorted(glob.glob(f'{point_src_path}/*.npy'))\n",
    "\n",
    "train_ls = []\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "for dat_typ in dat_typs:\n",
    "    # images_typ = [image for image in images if dat_typ in image]\n",
    "    scenes_typ = [scene for scene in scenes if dat_typ in scene]\n",
    "    \n",
    "    train_val, test = train_test_split(scenes_typ, test_size=0.2, shuffle=False, random_state=44)\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=44)\n",
    "\n",
    "    # train_ls += [train]\n",
    "    # val_ls += val\n",
    "    # test_ls += test\n",
    "    for j in train:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                train_ls.append(point)\n",
    "\n",
    "    for j in val:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                val_ls.append(point)\n",
    "\n",
    "    for j in test:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                test_ls.append(point)\n",
    "\n",
    "with open(save_path+'train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(train_ls)))\n",
    "    \n",
    "with open(save_path+'val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(val_ls)))\n",
    "\n",
    "with open(save_path+'test.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(test_ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = sorted(glob.glob('/data/NIA50/50-2/data/NIA50_real/*/label/*.json'))\n",
    "# # save_path = '/data/NIA50/50-2/data/mot_nia50/pvrcnn/labels/'\n",
    "\n",
    "# df = pd.DataFrame(columns = ['clip', 'CAR', 'PEDESTRIAN', 'MOTORCYCLE'])\n",
    "# rename = {'Adult': 'PEDESTRIAN', \n",
    "#           'Bus': 'CAR',\n",
    "#           'Car': 'CAR',\n",
    "#           'Kid': 'PEDESTRIAN',\n",
    "#           'Kickboard': 'MOTORCYCLE',\n",
    "#           'Large_Truck': 'CAR',\n",
    "#           'Light_Car': 'CAR',\n",
    "#           'Medium_Truck': 'CAR',\n",
    "#           'Mini_Bus': 'CAR',\n",
    "#           'Motorcycle': 'MOTORCYCLE',\n",
    "#           'Pedestrian': 'PEDESTRIAN',\n",
    "#           'SUV': 'CAR',\n",
    "#           'Small_Car': 'CAR',\n",
    "#           'Small_Truck': 'CAR',\n",
    "#           'Special_Vehicle': 'CAR',\n",
    "#           'Two_Wheeler': 'MOTORCYCLE',\n",
    "#           'Van': 'CAR'}\n",
    "\n",
    "# error_clips = []\n",
    "# for idx, label in enumerate(labels):\n",
    "#     try:\n",
    "#         clip = re.search('Suwon_[A-Z_0-9]+', label).group()\n",
    "#         frame = re.search('[0-9]+.json', label).group().replace('.json', '')\n",
    "        \n",
    "#         with open(label, 'r') as f:\n",
    "#             label = json.load(f)\n",
    "        \n",
    "#         class_list = []\n",
    "#         # label_txt = []\n",
    "#         for i in np.arange(len(label)):\n",
    "#             try:\n",
    "#                 class_ = label[i]['obj_type']\n",
    "#                 rename_class = [rename[class_]]\n",
    "#                 class_list.append(rename_class[0])\n",
    "#             except KeyError:\n",
    "#                 print('라벨링 에러 :', clip, frame, class_)\n",
    "        \n",
    "#         cnt = Counter(class_list)\n",
    "#         df.loc[idx] = [clip, cnt['CAR'], cnt['PEDESTRIAN'], cnt['MOTORCYCLE']]\n",
    "#         # with open(save_path+f'/{clip}_{frame}.txt', 'w') as f:\n",
    "#         #     f.write('\\n'.join(label_txt))\n",
    "        \n",
    "#     except ValueError:\n",
    "#         print('json 에러 :', clip, frame)\n",
    "#         error_clips.append(clip)\n",
    "        \n",
    "# df_ = df.groupby(['clip']).sum().reset_index()\n",
    "\n",
    "# error_clip_idx = []\n",
    "# for error_clip in list(set(error_clips)):\n",
    "#     idx = df_.loc[df_['clip'] == error_clip].index.values\n",
    "#     error_clip_idx.append(int(idx))\n",
    "\n",
    "# df_ = df_.drop(error_clip_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip1 = list(df_.loc[(df_['PEDESTRIAN'] >= 1) & (df_['MOTORCYCLE'] >= 1), 'clip'])\n",
    "# clip2 = list(df_.loc[(df_['PEDESTRIAN'] < 1) | (df_['MOTORCYCLE'] < 1), 'clip'])\n",
    "\n",
    "# train1, val1 = train_test_split(clip1, test_size=0.2, random_state=0)\n",
    "# train2, val2 = train_test_split(clip2, test_size=0.2, random_state=0)\n",
    "\n",
    "# train = sorted(train1 + train2); val = sorted(val1 + val2)\n",
    "# len(train), len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = '/data/NIA50/50-2/data/mot_nia50/pvrcnn/ImageSets/'\n",
    "\n",
    "# frames = [str(i).zfill(4) for i in np.arange(0, 10)]\n",
    "\n",
    "# train_list = []\n",
    "# for clip in train:\n",
    "#     for frame in frames:\n",
    "#         train_file = clip + f'_{frame}'\n",
    "#         train_list.append(train_file)\n",
    "\n",
    "# val_list = []\n",
    "# for clip in val:\n",
    "#     for frame in frames:\n",
    "#         val_file = clip + f'_{frame}'\n",
    "#         val_list.append(val_file)\n",
    "        \n",
    "\n",
    "# with open(save_path + 'train.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(train_list))           \n",
    "    \n",
    "# with open(save_path + 'val.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(val_list))           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiner310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "746d95b724613cc31ae9ea1c95fce8e51ec3ee7393c1b2a647745f061ae2ccda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
